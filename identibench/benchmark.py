"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/benchmark.ipynb.

# %% auto 0
__all__ = ['get_default_data_root', 'BenchmarkSpec', 'TrainingContext']

# %% ../nbs/benchmark.ipynb 2
import dataclasses
from typing import List, Optional, Callable, Dict, Any, Iterator, Tuple
from pathlib import Path
import os
import h5py
import numpy as np
import itertools # For chaining iterators

# %% ../nbs/benchmark.ipynb 4
def get_default_data_root() -> Path:
    """
    Returns the default root directory for datasets.

    Checks the 'IDENTIBENCH_DATA_ROOT' environment variable first,
    otherwise defaults to '~/.identibench_data'.
    """
    return Path(os.environ.get('IDENTIBENCH_DATA_ROOT', Path.home() / '.identibench_data'))


# %% ../nbs/benchmark.ipynb 5
@dataclasses.dataclass(frozen=True)
class BenchmarkSpec:
    """
    Specification for a single, standardized benchmark dataset configuration.

    Defines fixed parameters for dataset loading, preprocessing, and evaluation metric.
    Specific evaluation logic (simulation vs prediction, windowing) is handled
    by the benchmark execution function using parameters like init_window, pred_horizon, etc.
    """
    name: str # A unique name for this specific benchmark configuration.
    dataset_id: str # Identifier for the dataset (e.g., 'dummy'). Corresponds to the subdirectory name within the data root.
    u_cols: List[str] # List of column names for the input signals (u).
    y_cols: List[str] # List of column names for the output signals (y).
    x_cols: Optional[List[str]] = None # Optional list of column names for state inputs (x).
    download_func: Optional[Callable[[Path, bool], None]] = None # Function to download/prepare the raw dataset. `func(save_path, force_download)`
    metric_func: Optional[Callable[[np.ndarray, np.ndarray], float]] = None # Primary evaluation metric function. `func(y_true, y_pred)`

    # --- Parameters potentially used by benchmark execution functions ---
    # Note: Specific benchmark functions (e.g., for prediction) might require these to be set.
    init_window: Optional[int] = None # Number of initial steps potentially used for model initialization (simulation or prediction).
    pred_horizon: Optional[int] = None # The 'k' in k-step ahead prediction, used if the benchmark function performs prediction.
    pred_step: int = 1 # Step size for k-step ahead prediction, used if the benchmark function performs prediction.

    # Function to get data root
    data_root_func: Callable[[], Path] = get_default_data_root # Function that returns the root directory where datasets are stored.

    @property
    def data_root(self) -> Path:
        """Returns the evaluated data root path."""
        return self.data_root_func()

    @property
    def dataset_path(self) -> Path:
        """Returns the full path to the dataset directory."""
        return self.data_root / self.dataset_id

    def ensure_dataset_exists(self, force_download: bool = False) -> None:
        """
        Checks if the dataset exists locally, downloads it if not or if forced.

        Args:
            force_download: If True, download the dataset even if it exists locally.
        """
        dataset_path = self.dataset_path # Evaluate once
        if self.download_func is None:
            print(f"Warning: No download function specified for benchmark '{self.name}'. Assuming data exists at {dataset_path}")
            if not dataset_path.is_dir():
                 print(f"Warning: Dataset directory {dataset_path} not found.")
            return

        dataset_exists = dataset_path.is_dir()

        if not dataset_exists or force_download:
            print(f"Dataset for '{self.name}' {'not found' if not dataset_exists else 'download forced'}. Preparing dataset at {dataset_path}...")
            self.data_root.mkdir(parents=True, exist_ok=True) # Ensure parent exists
            try:
                self.download_func(dataset_path, force_download=force_download)
                print(f"Dataset '{self.name}' prepared successfully.")
            except Exception as e:
                print(f"Error preparing dataset '{self.name}': {e}")
                raise

# %% ../nbs/benchmark.ipynb 8
# Internal helper function for loading sequences (keep near TrainingContext)
def _load_sequences_from_files(
    file_paths: List[Path], # List of HDF5 file paths to load from.
    u_cols: List[str], # Input column names.
    y_cols: List[str], # Output column names.
    x_cols: Optional[List[str]], # Optional state column names.
    win_sz: Optional[int], # Window size (sequence length). If None, yield full sequences.
    stp_sz: Optional[int], # Step size for sliding window. If None or win_sz is None, use 1.
) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
    """
    Loads and yields sequences (u, y, x) from HDF5 files, applying windowing.
    """
    if not file_paths: return iter([]) # Handle empty list early

    if win_sz is not None and stp_sz is None:
        stp_sz = 1 # Default step size to 1 if windowing is enabled

    for file_path in file_paths:
        try:
            with h5py.File(file_path, 'r') as f:
                try:
                    u_data = np.stack([f[col][()] for col in u_cols], axis=-1).astype(np.float32)
                    y_data = np.stack([f[col][()] for col in y_cols], axis=-1).astype(np.float32)
                    x_data = np.stack([f[col][()] for col in x_cols], axis=-1).astype(np.float32) if x_cols else None
                except KeyError as e:
                    print(f"Warning: Column {e} not found in file {file_path}. Skipping file.")
                    continue

                seq_len = u_data.shape[0]
                if y_data.shape[0] != seq_len or (x_data is not None and x_data.shape[0] != seq_len):
                     print(f"Warning: Column length mismatch in {file_path}. Skipping file.")
                     continue

                if win_sz is None: # Yield full sequence
                    yield u_data, y_data, x_data
                else: # Yield windowed sequences
                    if win_sz > seq_len: continue # Skip if window larger than sequence
                    for i in range(0, seq_len - win_sz + 1, stp_sz):
                        yield (u_data[i : i + win_sz],
                               y_data[i : i + win_sz],
                               x_data[i : i + win_sz] if x_data is not None else None)
        except Exception as e:
            print(f"Error reading or processing file {file_path}: {e}")

# %% ../nbs/benchmark.ipynb 9
@dataclasses.dataclass(frozen=True)
class TrainingContext:
    """
    Context object passed to the user's training function (`build_predictor`).

    Holds the benchmark specification and user-defined training configurations.
    Provides methods to access training, validation, and combined train/validation
    data sequences lazily. Windowing/stepping for these sequences is controlled by
    parameters passed directly to the `get_..._sequences` methods or taken from `train_config`.
    """
    spec: BenchmarkSpec # The benchmark specification.
    train_config: Dict[str, Any] # User-provided training configuration dictionary (e.g., hyperparameters, seed, train_win_sz, etc.).

    def _get_file_paths(self, subset: str) -> List[Path]:
        """Gets sorted list of HDF5 files for a given subset directory."""
        subset_path = self.spec.dataset_path / subset
        if not subset_path.is_dir():
            # Don't print warning here, let the caller decide based on context
            return []
        return sorted(list(subset_path.glob('*.hdf5')))

    def _get_sequences_from_subset(self, subset: str, win_sz: Optional[int], stp_sz: Optional[int]
                                  ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """Loads sequences for a specific subset directory."""
        file_paths = self._get_file_paths(subset)
        if not file_paths:
             print(f"Warning: No HDF5 files found in {self.spec.dataset_path / subset}. Returning empty iterator.")
             return iter([])

        return _load_sequences_from_files(
            file_paths=file_paths,
            u_cols=self.spec.u_cols,
            y_cols=self.spec.y_cols,
            x_cols=self.spec.x_cols,
            win_sz=win_sz,
            stp_sz=stp_sz,
        )

    def get_train_sequences(self,
                            win_sz: Optional[int] = None, # Window size for training sequences.
                            stp_sz: Optional[int] = None, # Step size for training sequences.
                           ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """
        Returns a lazy iterator yielding (u, y, x) tuples for the 'train' subset.

        Window/step size taken from args or 'train_win_sz'/'train_stp_sz' in `train_config`.
        """
        win_sz = win_sz if win_sz is not None else self.train_config.get('train_win_sz')
        stp_sz = stp_sz if stp_sz is not None else self.train_config.get('train_stp_sz')
        return self._get_sequences_from_subset('train', win_sz, stp_sz)

    def get_valid_sequences(self,
                            win_sz: Optional[int] = None, # Window size for validation sequences.
                            stp_sz: Optional[int] = None, # Step size for validation sequences.
                           ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """
        Returns a lazy iterator yielding (u, y, x) tuples for the 'valid' subset.

        Window/step size taken from args or 'valid_win_sz'/'valid_stp_sz' in `train_config`.
        """
        win_sz = win_sz if win_sz is not None else self.train_config.get('valid_win_sz')
        stp_sz = stp_sz if stp_sz is not None else self.train_config.get('valid_stp_sz')
        return self._get_sequences_from_subset('valid', win_sz, stp_sz)

    def get_train_valid_sequences(self,
                                  win_sz: Optional[int] = None, # Window size for train_valid sequences.
                                  stp_sz: Optional[int] = None, # Step size for train_valid sequences.
                                 ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """
        Returns a lazy iterator yielding (u, y, x) tuples for combined training and validation.

        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.
        If not, it loads data from 'train' and 'valid' subsets sequentially.
        Window/step size taken from args or 'train_valid_win_sz'/'train_valid_stp_sz' in `train_config`.
        If falling back to train+valid, uses 'train_win_sz'/'train_stp_sz' and 'valid_win_sz'/'valid_stp_sz'.
        """
        # Determine window/step sizes based on args or train_config
        tv_win_sz = win_sz if win_sz is not None else self.train_config.get('train_valid_win_sz')
        tv_stp_sz = stp_sz if stp_sz is not None else self.train_config.get('train_valid_stp_sz')

        # Try loading from 'train_valid' directory first
        train_valid_files = self._get_file_paths('train_valid')
        if train_valid_files:
            # print("Loading from train_valid directory.") # Optional debug print
            return _load_sequences_from_files(
                file_paths=train_valid_files, u_cols=self.spec.u_cols, y_cols=self.spec.y_cols,
                x_cols=self.spec.x_cols, win_sz=tv_win_sz, stp_sz=tv_stp_sz
            )
        else:
            # print("train_valid directory not found or empty. Combining train and valid.") # Optional debug print
            # Fallback: load from 'train' and 'valid' separately and chain them
            # Use specific train/valid window/step sizes if train_valid ones weren't provided
            train_win = tv_win_sz if tv_win_sz is not None else self.train_config.get('train_win_sz')
            train_stp = tv_stp_sz if tv_stp_sz is not None else self.train_config.get('train_stp_sz')
            valid_win = tv_win_sz if tv_win_sz is not None else self.train_config.get('valid_win_sz')
            valid_stp = tv_stp_sz if tv_stp_sz is not None else self.train_config.get('valid_stp_sz')

            train_iter = self._get_sequences_from_subset('train', train_win, train_stp)
            valid_iter = self._get_sequences_from_subset('valid', valid_win, valid_stp)
            return itertools.chain(train_iter, valid_iter)
