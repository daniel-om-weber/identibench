"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/core.ipynb.

# %% auto 0
__all__ = ['get_default_data_root', 'BenchmarkSpec', 'TrainingContext']

# %% ../nbs/core.ipynb 2
from typing import List, Optional, Callable, Dict, Any, Iterator, Tuple
from pathlib import Path
import os
import h5py
import numpy as np
import itertools
import identibench.metrics

# %% ../nbs/core.ipynb 4
def get_default_data_root() -> Path:
    """
    Returns the default root directory for datasets.

    Checks the 'IDENTIBENCH_DATA_ROOT' environment variable first,
    otherwise defaults to '~/.identibench_data'.
    """
    return Path(os.environ.get('IDENTIBENCH_DATA_ROOT', Path.home() / '.identibench_data'))


# %% ../nbs/core.ipynb 5
# Forward declaration needed for type hint in BenchmarkSpec.test_func
class BenchmarkSpec: pass 

# %% ../nbs/core.ipynb 6
class BenchmarkSpec:
    """
    Specification for a single, standardized benchmark dataset configuration.

    Defines fixed parameters for dataset loading, preprocessing, evaluation metric,
    and potentially a custom testing function. Specific evaluation logic 
    (simulation vs prediction, windowing) is handled by the benchmark execution function 
    or the custom test_func.
    """
    # Explicit __init__ for nbdev documentation compatibility
    def __init__(self, 
                 name: str, # Unique name identifying this specific benchmark task (e.g., 'silverbox_sim_rmse').
                 dataset_id: str, # Identifier for the raw dataset source (e.g., 'silverbox'), corresponds to subdirectory name.
                 u_cols: List[str], # List of column names for the input signals (u).
                 y_cols: List[str], # List of column names for the output signals (y).
                 metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric for final test evaluation. `func(y_true, y_pred)`.
                 x_cols: Optional[List[str]] = None, # Optional list of column names for state inputs (x).
                 sampling_time: Optional[float] = None, # Optional sampling time (in seconds) if constant for the dataset.
                 download_func: Optional[Callable[[Path, bool], None]] = None, # Function to download/prepare the raw dataset. `func(save_path, force_download)`
                 test_func: Optional[Callable[[BenchmarkSpec, Callable[[np.ndarray], np.ndarray]], Dict[str, Any]]] = None, # Optional custom function to perform testing and return results dict. Takes spec and predictor. Overrides default test logic.
                 init_window: Optional[int] = None, # Number of initial steps potentially used for model initialization (simulation or prediction).
                 pred_horizon: Optional[int] = None, # The 'k' in k-step ahead prediction, used if the benchmark function performs prediction.
                 pred_step: int = 1, # Step size for k-step ahead prediction, used if the benchmark function performs prediction.
                 data_root_func: Callable[[], Path] = get_default_data_root # Function that returns the root directory where datasets are stored.
                ):
        # Standard attribute assignment
        self.name = name
        self.dataset_id = dataset_id
        self.u_cols = u_cols
        self.y_cols = y_cols
        self.metric_func = metric_func # Now mandatory
        self.x_cols = x_cols
        self.sampling_time = sampling_time
        self.download_func = download_func
        self.test_func = test_func
        self.init_window = init_window
        self.pred_horizon = pred_horizon
        self.pred_step = pred_step
        self.data_root_func = data_root_func

    @property
    def data_root(self) -> Path:
        """Returns the evaluated data root path."""
        return self.data_root_func() 

    @property
    def dataset_path(self) -> Path:
        """Returns the full path to the dataset directory."""
        return self.data_root / self.dataset_id

    # Reinstated ensure_dataset_exists method
    def ensure_dataset_exists(self, force_download: bool = False) -> None:
        """
        Checks if the dataset exists locally, downloads it if not or if forced.

        Args:
            force_download: If True, download the dataset even if it exists locally.
        """
        dataset_path = self.dataset_path 
        download_func = self.download_func 
        if download_func is None:
            print(f"Warning: No download function specified for benchmark '{self.name}'. Cannot ensure data exists at {dataset_path}")
            # Check existence even if we can't download
            if not dataset_path.is_dir():
                 print(f"Warning: Dataset directory {dataset_path} not found.")
            return

        dataset_exists = dataset_path.is_dir()

        if not dataset_exists or force_download:
            print(f"Dataset for '{self.name}' {'not found' if not dataset_exists else 'download forced'}. Preparing dataset at {dataset_path}...")
            self.data_root.mkdir(parents=True, exist_ok=True) 
            try:
                download_func(dataset_path, force_download) 
                print(f"Dataset '{self.name}' prepared successfully.")
            except Exception as e:
                print(f"Error preparing dataset '{self.name}': {e}")
                raise
        else:
             # Optionally print message if dataset already exists and not forced
             # print(f"Dataset for '{self.name}' found at {dataset_path}.")
             pass

# %% ../nbs/core.ipynb 15
# Internal helper function for loading raw sequences (no windowing)
def _load_raw_sequences_from_files(
    file_paths: List[Path], # List of HDF5 file paths to load from.
    u_cols: List[str], # Input column names.
    y_cols: List[str], # Output column names.
    x_cols: Optional[List[str]], # Optional state column names.
) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
    """
    Loads and yields full sequences (u, y, x) from HDF5 files.
    """
    if not file_paths: return iter([]) 

    for file_path in file_paths:
        try:
            with h5py.File(file_path, 'r') as f:
                try:
                    u_data = np.stack([f[col][()] for col in u_cols], axis=-1).astype(np.float32)
                    y_data = np.stack([f[col][()] for col in y_cols], axis=-1).astype(np.float32)
                    x_data = np.stack([f[col][()] for col in x_cols], axis=-1).astype(np.float32) if x_cols else None
                except KeyError as e:
                    print(f"Warning: Column {e} not found in file {file_path}. Skipping file.")
                    continue

                seq_len = u_data.shape[0]
                if y_data.shape[0] != seq_len or (x_data is not None and x_data.shape[0] != seq_len):
                     print(f"Warning: Column length mismatch in {file_path}. Skipping file.")
                     continue

                yield u_data, y_data, x_data 
        except Exception as e:
            print(f"Error reading or processing file {file_path}: {e}")

# %% ../nbs/core.ipynb 16
class TrainingContext:
    """
    Context object passed to the user's training function (`build_predictor`).

    Holds the benchmark specification, hyperparameters, and seed.
    Provides methods to access the raw, full-length training and validation data sequences.
    Windowing/batching for training must be handled within the user's `build_predictor` function.
    """
    # Explicit __init__ for nbdev documentation compatibility
    def __init__(self, 
                 spec: BenchmarkSpec, # The benchmark specification.
                 hyperparameters: Dict[str, Any], # User-provided dictionary containing model and training hyperparameters.
                 seed: Optional[int] = None # Optional random seed for reproducibility.
                ):
        # Standard attribute assignment
        self.spec = spec
        self.hyperparameters = hyperparameters
        self.seed = seed

    # --- Data Access Methods ---

    def _get_file_paths(self, subset: str) -> List[Path]:
        """Gets sorted list of HDF5 files for a given subset directory."""
        subset_path = self.spec.dataset_path / subset
        if not subset_path.is_dir():
            return []
        return sorted(list(subset_path.glob('*.hdf5')))

    def _get_sequences_from_subset(self, subset: str
                                  ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """Loads raw sequences for a specific subset directory."""
        file_paths = self._get_file_paths(subset)
        if not file_paths:
             print(f"Warning: No HDF5 files found in {self.spec.dataset_path / subset}. Returning empty iterator.")
             return iter([])

        return _load_raw_sequences_from_files(
            file_paths=file_paths,
            u_cols=self.spec.u_cols,
            y_cols=self.spec.y_cols,
            x_cols=self.spec.x_cols,
        )

    def get_train_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """Returns a lazy iterator yielding raw (u, y, x) tuples for the 'train' subset."""
        return self._get_sequences_from_subset('train')

    def get_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """Returns a lazy iterator yielding raw (u, y, x) tuples for the 'valid' subset."""
        return self._get_sequences_from_subset('valid')

    def get_train_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:
        """
        Returns a lazy iterator yielding raw (u, y, x) tuples for combined training and validation.

        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.
        If not, it loads data from 'train' and 'valid' subsets sequentially.
        """
        train_valid_files = self._get_file_paths('train_valid')
        if train_valid_files:
            return _load_raw_sequences_from_files(
                file_paths=train_valid_files, u_cols=self.spec.u_cols, y_cols=self.spec.y_cols,
                x_cols=self.spec.x_cols
            )
        else:
            train_iter = self._get_sequences_from_subset('train')
            valid_iter = self._get_sequences_from_subset('valid')
            return itertools.chain(train_iter, valid_iter)
