{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import dataclasses\n",
    "from typing import List, Optional, Callable, Dict, Any, Iterator, Tuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import itertools # For chaining iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "import time # For testing modification times\n",
    "from functools import partial\n",
    "from fastcore.test import test_eq, test_ne, test_fail, test_close # Import nbdev testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_default_data_root() -> Path:\n",
    "    \"\"\"\n",
    "    Returns the default root directory for datasets.\n",
    "\n",
    "    Checks the 'IDENTIBENCH_DATA_ROOT' environment variable first,\n",
    "    otherwise defaults to '~/.identibench_data'.\n",
    "    \"\"\"\n",
    "    return Path(os.environ.get('IDENTIBENCH_DATA_ROOT', Path.home() / '.identibench_data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class BenchmarkSpec:\n",
    "    \"\"\"\n",
    "    Specification for a single, standardized benchmark dataset configuration.\n",
    "\n",
    "    Defines fixed parameters for dataset loading, preprocessing, and evaluation metric.\n",
    "    Specific evaluation logic (simulation vs prediction, windowing) is handled\n",
    "    by the benchmark execution function using parameters like init_window, pred_horizon, etc.\n",
    "    \"\"\"\n",
    "    name: str # A unique name for this specific benchmark configuration.\n",
    "    dataset_id: str # Identifier for the dataset (e.g., 'dummy'). Corresponds to the subdirectory name within the data root.\n",
    "    u_cols: List[str] # List of column names for the input signals (u).\n",
    "    y_cols: List[str] # List of column names for the output signals (y).\n",
    "    x_cols: Optional[List[str]] = None # Optional list of column names for state inputs (x).\n",
    "    download_func: Optional[Callable[[Path, bool], None]] = None # Function to download/prepare the raw dataset. `func(save_path, force_download)`\n",
    "    metric_func: Optional[Callable[[np.ndarray, np.ndarray], float]] = None # Primary evaluation metric function. `func(y_true, y_pred)`\n",
    "\n",
    "    # --- Parameters potentially used by benchmark execution functions ---\n",
    "    # Note: Specific benchmark functions (e.g., for prediction) might require these to be set.\n",
    "    init_window: Optional[int] = None # Number of initial steps potentially used for model initialization (simulation or prediction).\n",
    "    pred_horizon: Optional[int] = None # The 'k' in k-step ahead prediction, used if the benchmark function performs prediction.\n",
    "    pred_step: int = 1 # Step size for k-step ahead prediction, used if the benchmark function performs prediction.\n",
    "\n",
    "    # Function to get data root\n",
    "    data_root_func: Callable[[], Path] = get_default_data_root # Function that returns the root directory where datasets are stored.\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        \"\"\"Returns the evaluated data root path.\"\"\"\n",
    "        return self.data_root_func()\n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the dataset directory.\"\"\"\n",
    "        return self.data_root / self.dataset_id\n",
    "\n",
    "    def ensure_dataset_exists(self, force_download: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Checks if the dataset exists locally, downloads it if not or if forced.\n",
    "\n",
    "        Args:\n",
    "            force_download: If True, download the dataset even if it exists locally.\n",
    "        \"\"\"\n",
    "        dataset_path = self.dataset_path # Evaluate once\n",
    "        if self.download_func is None:\n",
    "            print(f\"Warning: No download function specified for benchmark '{self.name}'. Assuming data exists at {dataset_path}\")\n",
    "            if not dataset_path.is_dir():\n",
    "                 print(f\"Warning: Dataset directory {dataset_path} not found.\")\n",
    "            return\n",
    "\n",
    "        dataset_exists = dataset_path.is_dir()\n",
    "\n",
    "        if not dataset_exists or force_download:\n",
    "            print(f\"Dataset for '{self.name}' {'not found' if not dataset_exists else 'download forced'}. Preparing dataset at {dataset_path}...\")\n",
    "            self.data_root.mkdir(parents=True, exist_ok=True) # Ensure parent exists\n",
    "            try:\n",
    "                self.download_func(dataset_path, force_download=force_download)\n",
    "                print(f\"Dataset '{self.name}' prepared successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing dataset '{self.name}': {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal dummy loader - needed for tests below\n",
    "def _dummy_dataset_loader(\n",
    "    save_path: Path, # Directory where the dummy dataset files will be written\n",
    "    force_download: bool = False, # Argument for interface compatibility\n",
    "    create_train_valid_dir: bool = False # If True, create a 'train_valid' subdir as well\n",
    "    ):\n",
    "    \"\"\"Creates a dummy dataset structure with minimal HDF5 files for testing.\"\"\"\n",
    "    save_path = Path(save_path)\n",
    "    if save_path.is_dir() and not force_download: return\n",
    "\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    seq_len = 50\n",
    "    subdirs = ['train', 'valid', 'test']\n",
    "    if create_train_valid_dir:\n",
    "        subdirs.append('train_valid')\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = save_path / subdir\n",
    "        subdir_path.mkdir(exist_ok=True)\n",
    "        n_files = 1 if subdir == 'train_valid' else 2 # Create fewer files in train_valid for testing differentiation\n",
    "        for i in range(n_files):\n",
    "            dummy_file_path = subdir_path / f'{subdir}_{i}.hdf5'\n",
    "            try:\n",
    "                with h5py.File(dummy_file_path, 'w') as f:\n",
    "                    f.create_dataset('u0', data=np.random.rand(seq_len).astype(np.float32))\n",
    "                    f.create_dataset('u1', data=np.random.rand(seq_len).astype(np.float32))\n",
    "                    f.create_dataset('y0', data=np.random.rand(seq_len).astype(np.float32))\n",
    "                    f.attrs['fs'] = 10.0\n",
    "            except Exception as e: print(f\"Failed to create dummy file {dummy_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_spec_default' not found. Preparing dataset at _temp_identibench_data_spec_test/_dummy_default...\n",
      "Dataset '_spec_default' prepared successfully.\n",
      "Dataset for '_spec_default' download forced. Preparing dataset at _temp_identibench_data_spec_test/_dummy_default...\n",
      "Dataset '_spec_default' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Tests for BenchmarkSpec ---\n",
    "\n",
    "# Setup for BenchmarkSpec Tests\n",
    "_test_data_dir_spec = Path('./_temp_identibench_data_spec_test')\n",
    "shutil.rmtree(_test_data_dir_spec, ignore_errors=True)\n",
    "def _get_test_data_root_spec(): return _test_data_dir_spec\n",
    "\n",
    "# Test basic initialization and defaults\n",
    "_spec_default = BenchmarkSpec(\n",
    "    name='_spec_default', dataset_id='_dummy_default',\n",
    "    u_cols=['u0'], y_cols=['y0'], download_func=_dummy_dataset_loader,\n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_default.init_window, None)\n",
    "test_eq(_spec_default.pred_horizon, None)\n",
    "test_eq(_spec_default.pred_step, 1)\n",
    "\n",
    "# Test initialization with prediction-related parameters\n",
    "_spec_pred_params = BenchmarkSpec(\n",
    "    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n",
    "    u_cols=['u0'], y_cols=['y0'], download_func=_dummy_dataset_loader,\n",
    "    init_window=20, pred_horizon=5, pred_step=2,\n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_pred_params.init_window, 20)\n",
    "test_eq(_spec_pred_params.pred_horizon, 5)\n",
    "test_eq(_spec_pred_params.pred_step, 2)\n",
    "\n",
    "# Test ensure_dataset_exists (remains the same logic)\n",
    "_spec_default.ensure_dataset_exists()\n",
    "_dataset_path = _spec_default.dataset_path\n",
    "test_eq(_dataset_path.is_dir(), True)\n",
    "test_eq((_dataset_path / 'train' / 'train_0.hdf5').is_file(), True)\n",
    "_mtime_before = (_dataset_path / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1)\n",
    "_spec_default.ensure_dataset_exists() # Should skip\n",
    "_mtime_after_skip = (_dataset_path / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_eq(_mtime_before, _mtime_after_skip)\n",
    "time.sleep(0.1)\n",
    "_spec_default.ensure_dataset_exists(force_download=True) # Should run\n",
    "_mtime_after_force = (_dataset_path / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_ne(_mtime_before, _mtime_after_force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Internal helper function for loading sequences (keep near TrainingContext)\n",
    "def _load_sequences_from_files(\n",
    "    file_paths: List[Path], # List of HDF5 file paths to load from.\n",
    "    u_cols: List[str], # Input column names.\n",
    "    y_cols: List[str], # Output column names.\n",
    "    x_cols: Optional[List[str]], # Optional state column names.\n",
    "    win_sz: Optional[int], # Window size (sequence length). If None, yield full sequences.\n",
    "    stp_sz: Optional[int], # Step size for sliding window. If None or win_sz is None, use 1.\n",
    ") -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "    \"\"\"\n",
    "    Loads and yields sequences (u, y, x) from HDF5 files, applying windowing.\n",
    "    \"\"\"\n",
    "    if not file_paths: return iter([]) # Handle empty list early\n",
    "\n",
    "    if win_sz is not None and stp_sz is None:\n",
    "        stp_sz = 1 # Default step size to 1 if windowing is enabled\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                try:\n",
    "                    u_data = np.stack([f[col][()] for col in u_cols], axis=-1).astype(np.float32)\n",
    "                    y_data = np.stack([f[col][()] for col in y_cols], axis=-1).astype(np.float32)\n",
    "                    x_data = np.stack([f[col][()] for col in x_cols], axis=-1).astype(np.float32) if x_cols else None\n",
    "                except KeyError as e:\n",
    "                    print(f\"Warning: Column {e} not found in file {file_path}. Skipping file.\")\n",
    "                    continue\n",
    "\n",
    "                seq_len = u_data.shape[0]\n",
    "                if y_data.shape[0] != seq_len or (x_data is not None and x_data.shape[0] != seq_len):\n",
    "                     print(f\"Warning: Column length mismatch in {file_path}. Skipping file.\")\n",
    "                     continue\n",
    "\n",
    "                if win_sz is None: # Yield full sequence\n",
    "                    yield u_data, y_data, x_data\n",
    "                else: # Yield windowed sequences\n",
    "                    if win_sz > seq_len: continue # Skip if window larger than sequence\n",
    "                    for i in range(0, seq_len - win_sz + 1, stp_sz):\n",
    "                        yield (u_data[i : i + win_sz],\n",
    "                               y_data[i : i + win_sz],\n",
    "                               x_data[i : i + win_sz] if x_data is not None else None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TrainingContext:\n",
    "    \"\"\"\n",
    "    Context object passed to the user's training function (`build_predictor`).\n",
    "\n",
    "    Holds the benchmark specification and user-defined training configurations.\n",
    "    Provides methods to access training, validation, and combined train/validation\n",
    "    data sequences lazily. Windowing/stepping for these sequences is controlled by\n",
    "    parameters passed directly to the `get_..._sequences` methods or taken from `train_config`.\n",
    "    \"\"\"\n",
    "    spec: BenchmarkSpec # The benchmark specification.\n",
    "    train_config: Dict[str, Any] # User-provided training configuration dictionary (e.g., hyperparameters, seed, train_win_sz, etc.).\n",
    "\n",
    "    def _get_file_paths(self, subset: str) -> List[Path]:\n",
    "        \"\"\"Gets sorted list of HDF5 files for a given subset directory.\"\"\"\n",
    "        subset_path = self.spec.dataset_path / subset\n",
    "        if not subset_path.is_dir():\n",
    "            # Don't print warning here, let the caller decide based on context\n",
    "            return []\n",
    "        return sorted(list(subset_path.glob('*.hdf5')))\n",
    "\n",
    "    def _get_sequences_from_subset(self, subset: str, win_sz: Optional[int], stp_sz: Optional[int]\n",
    "                                  ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Loads sequences for a specific subset directory.\"\"\"\n",
    "        file_paths = self._get_file_paths(subset)\n",
    "        if not file_paths:\n",
    "             print(f\"Warning: No HDF5 files found in {self.spec.dataset_path / subset}. Returning empty iterator.\")\n",
    "             return iter([])\n",
    "\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=file_paths,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "            win_sz=win_sz,\n",
    "            stp_sz=stp_sz,\n",
    "        )\n",
    "\n",
    "    def get_train_sequences(self,\n",
    "                            win_sz: Optional[int] = None, # Window size for training sequences.\n",
    "                            stp_sz: Optional[int] = None, # Step size for training sequences.\n",
    "                           ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding (u, y, x) tuples for the 'train' subset.\n",
    "\n",
    "        Window/step size taken from args or 'train_win_sz'/'train_stp_sz' in `train_config`.\n",
    "        \"\"\"\n",
    "        win_sz = win_sz if win_sz is not None else self.train_config.get('train_win_sz')\n",
    "        stp_sz = stp_sz if stp_sz is not None else self.train_config.get('train_stp_sz')\n",
    "        return self._get_sequences_from_subset('train', win_sz, stp_sz)\n",
    "\n",
    "    def get_valid_sequences(self,\n",
    "                            win_sz: Optional[int] = None, # Window size for validation sequences.\n",
    "                            stp_sz: Optional[int] = None, # Step size for validation sequences.\n",
    "                           ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding (u, y, x) tuples for the 'valid' subset.\n",
    "\n",
    "        Window/step size taken from args or 'valid_win_sz'/'valid_stp_sz' in `train_config`.\n",
    "        \"\"\"\n",
    "        win_sz = win_sz if win_sz is not None else self.train_config.get('valid_win_sz')\n",
    "        stp_sz = stp_sz if stp_sz is not None else self.train_config.get('valid_stp_sz')\n",
    "        return self._get_sequences_from_subset('valid', win_sz, stp_sz)\n",
    "\n",
    "    def get_train_valid_sequences(self,\n",
    "                                  win_sz: Optional[int] = None, # Window size for train_valid sequences.\n",
    "                                  stp_sz: Optional[int] = None, # Step size for train_valid sequences.\n",
    "                                 ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding (u, y, x) tuples for combined training and validation.\n",
    "\n",
    "        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.\n",
    "        If not, it loads data from 'train' and 'valid' subsets sequentially.\n",
    "        Window/step size taken from args or 'train_valid_win_sz'/'train_valid_stp_sz' in `train_config`.\n",
    "        If falling back to train+valid, uses 'train_win_sz'/'train_stp_sz' and 'valid_win_sz'/'valid_stp_sz'.\n",
    "        \"\"\"\n",
    "        # Determine window/step sizes based on args or train_config\n",
    "        tv_win_sz = win_sz if win_sz is not None else self.train_config.get('train_valid_win_sz')\n",
    "        tv_stp_sz = stp_sz if stp_sz is not None else self.train_config.get('train_valid_stp_sz')\n",
    "\n",
    "        # Try loading from 'train_valid' directory first\n",
    "        train_valid_files = self._get_file_paths('train_valid')\n",
    "        if train_valid_files:\n",
    "            # print(\"Loading from train_valid directory.\") # Optional debug print\n",
    "            return _load_sequences_from_files(\n",
    "                file_paths=train_valid_files, u_cols=self.spec.u_cols, y_cols=self.spec.y_cols,\n",
    "                x_cols=self.spec.x_cols, win_sz=tv_win_sz, stp_sz=tv_stp_sz\n",
    "            )\n",
    "        else:\n",
    "            # print(\"train_valid directory not found or empty. Combining train and valid.\") # Optional debug print\n",
    "            # Fallback: load from 'train' and 'valid' separately and chain them\n",
    "            # Use specific train/valid window/step sizes if train_valid ones weren't provided\n",
    "            train_win = tv_win_sz if tv_win_sz is not None else self.train_config.get('train_win_sz')\n",
    "            train_stp = tv_stp_sz if tv_stp_sz is not None else self.train_config.get('train_stp_sz')\n",
    "            valid_win = tv_win_sz if tv_win_sz is not None else self.train_config.get('valid_win_sz')\n",
    "            valid_stp = tv_stp_sz if tv_stp_sz is not None else self.train_config.get('valid_stp_sz')\n",
    "\n",
    "            train_iter = self._get_sequences_from_subset('train', train_win, train_stp)\n",
    "            valid_iter = self._get_sequences_from_subset('valid', valid_win, valid_stp)\n",
    "            return itertools.chain(train_iter, valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_dummy_ctx_base' not found. Preparing dataset at _temp_identibench_data_ctx_test/_dummy_dataset_ctx_base...\n",
      "Dataset '_dummy_ctx_base' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Tests for TrainingContext ---\n",
    "\n",
    "# Setup for TrainingContext Tests\n",
    "_test_data_dir_ctx = Path('./_temp_identibench_data_ctx_test')\n",
    "_test_data_dir_ctx_tv = Path('./_temp_identibench_data_ctx_tv_test') # Separate dir for train_valid test\n",
    "shutil.rmtree(_test_data_dir_ctx, ignore_errors=True)\n",
    "shutil.rmtree(_test_data_dir_ctx_tv, ignore_errors=True)\n",
    "def _get_test_data_root_ctx(): return _test_data_dir_ctx\n",
    "def _get_test_data_root_ctx_tv(): return _test_data_dir_ctx_tv\n",
    "\n",
    "# --- Create base dummy data (no train_valid dir) ---\n",
    "_dummy_spec_ctx_base = BenchmarkSpec(\n",
    "    name='_dummy_ctx_base', dataset_id='_dummy_dataset_ctx_base',\n",
    "    u_cols=['u0', 'u1'], y_cols=['y0'],\n",
    "    download_func=partial(_dummy_dataset_loader,create_train_valid_dir=False), # Explicitly False\n",
    "    data_root_func=_get_test_data_root_ctx, init_window=10\n",
    ")\n",
    "_dummy_spec_ctx_base.ensure_dataset_exists()\n",
    "\n",
    "_seq_len = 50 # From dummy loader\n",
    "_n_files_train_valid = 2 # Files per subset (train, valid) in base dummy loader\n",
    "_n_files_tv_dir = 1 # Files in train_valid dir in tv dummy loader\n",
    "\n",
    "# Test context initialization (remains the same)\n",
    "_train_config_base = {'seed': 42, 'lr': 0.01}\n",
    "_ctx = TrainingContext(spec=_dummy_spec_ctx_base, train_config=_train_config_base)\n",
    "test_eq(_ctx.spec, _dummy_spec_ctx_base)\n",
    "test_eq(_ctx.train_config, _train_config_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Data Loading Methods (Train/Valid remain similar) ---\n",
    "\n",
    "# Test get_train_sequences (window/step from train_config)\n",
    "_train_config_windowed = {**_train_config_base, 'train_win_sz': 10, 'train_stp_sz': 5}\n",
    "_ctx_windowed_cfg = TrainingContext(spec=_dummy_spec_ctx_base, train_config=_train_config_windowed)\n",
    "_train_sequences_cfg = list(_ctx_windowed_cfg.get_train_sequences())\n",
    "\n",
    "_expected_train_win_sz = 10\n",
    "_expected_train_stp_sz = 5\n",
    "_expected_n_train_windows_per_file = ( (_seq_len - _expected_train_win_sz) // _expected_train_stp_sz ) + 1\n",
    "test_eq(len(_train_sequences_cfg), _n_files_train_valid * _expected_n_train_windows_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test get_train_valid_sequences ---\n",
    "\n",
    "# Case 1: train_valid directory DOES NOT exist (use _dummy_spec_ctx_base)\n",
    "_train_config_tv_fallback = {\n",
    "    **_train_config_base,\n",
    "    'train_win_sz': 10, 'train_stp_sz': 5, # Used for train part\n",
    "    'valid_win_sz': 12, 'valid_stp_sz': 6  # Used for valid part\n",
    "}\n",
    "_ctx_tv_fallback = TrainingContext(spec=_dummy_spec_ctx_base, train_config=_train_config_tv_fallback)\n",
    "_tv_sequences_fallback = list(_ctx_tv_fallback.get_train_valid_sequences())\n",
    "\n",
    "_expected_n_train_win = ( (_seq_len - 10) // 5 ) + 1\n",
    "_expected_n_valid_win = ( (_seq_len - 12) // 6 ) + 1\n",
    "# Total sequences = (files_in_train * windows_per_train_file) + (files_in_valid * windows_per_valid_file)\n",
    "test_eq(len(_tv_sequences_fallback), _n_files_train_valid * _expected_n_train_win + _n_files_train_valid * _expected_n_valid_win)\n",
    "# Check shapes of first train window and first valid window (which occurs after all train windows)\n",
    "_u_tv_fb_train, _y_tv_fb_train, _ = _tv_sequences_fallback[0]\n",
    "test_eq(_u_tv_fb_train.shape[0], 10) # train_win_sz\n",
    "_u_tv_fb_valid, _y_tv_fb_valid, _ = _tv_sequences_fallback[_n_files_train_valid * _expected_n_train_win]\n",
    "test_eq(_u_tv_fb_valid.shape[0], 12) # valid_win_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_dummy_ctx_tv' not found. Preparing dataset at _temp_identibench_data_ctx_tv_test/_dummy_dataset_ctx_tv...\n",
      "Dataset '_dummy_ctx_tv' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Create dummy data WITH train_valid dir ---\n",
    "_dummy_spec_ctx_tv = BenchmarkSpec(\n",
    "    name='_dummy_ctx_tv', dataset_id='_dummy_dataset_ctx_tv',\n",
    "    u_cols=['u0', 'u1'], y_cols=['y0'],\n",
    "    download_func=partial(_dummy_dataset_loader,create_train_valid_dir=True), # Explicitly True\n",
    "    data_root_func=_get_test_data_root_ctx_tv, init_window=10\n",
    ")\n",
    "_dummy_spec_ctx_tv.ensure_dataset_exists()\n",
    "\n",
    "\n",
    "# Case 2: train_valid directory DOES exist (use _dummy_spec_ctx_tv)\n",
    "_train_config_tv_direct = {**_train_config_base, 'train_valid_win_sz': 11, 'train_valid_stp_sz': 4}\n",
    "_ctx_tv_direct = TrainingContext(spec=_dummy_spec_ctx_tv, train_config=_train_config_tv_direct)\n",
    "_tv_sequences_direct = list(_ctx_tv_direct.get_train_valid_sequences())\n",
    "\n",
    "_expected_tv_win_sz = 11\n",
    "_expected_tv_stp_sz = 4\n",
    "_expected_n_tv_windows_per_file = ( (_seq_len - _expected_tv_win_sz) // _expected_tv_stp_sz ) + 1\n",
    "# Total sequences = files_in_tv_dir * windows_per_tv_file\n",
    "test_eq(len(_tv_sequences_direct), _n_files_tv_dir * _expected_n_tv_windows_per_file)\n",
    "_u_tv_direct, _y_tv_direct, _ = _tv_sequences_direct[0]\n",
    "test_eq(_u_tv_direct.shape[0], _expected_tv_win_sz)\n",
    "\n",
    "# Case 3: train_valid exists, but window/step passed directly\n",
    "_tv_sequences_direct_args = list(_ctx_tv_direct.get_train_valid_sequences(win_sz=13, stp_sz=3))\n",
    "_expected_tv_win_sz_arg = 13\n",
    "_expected_tv_stp_sz_arg = 3\n",
    "_expected_n_tv_windows_per_file_arg = ( (_seq_len - _expected_tv_win_sz_arg) // _expected_tv_stp_sz_arg ) + 1\n",
    "test_eq(len(_tv_sequences_direct_args), _n_files_tv_dir * _expected_n_tv_windows_per_file_arg)\n",
    "_u_tv_direct_arg, _, _ = _tv_sequences_direct_args[0]\n",
    "test_eq(_u_tv_direct_arg.shape[0], _expected_tv_win_sz_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
