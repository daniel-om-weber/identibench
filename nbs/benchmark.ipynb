{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Optional, Callable, Dict, Any, Iterator, Tuple, Union\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "from identibench.utils import get_default_data_root,_load_sequences_from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq, test_ne# Import nbdev testing functions\n",
    "import identibench.metrics\n",
    "from identibench.utils import _dummy_dataset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _test_simulation(specs, model):\n",
    "    test_dir = specs.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "\n",
    "    if not test_files:\n",
    "        raise RuntimeError(f\"No test files found in {test_dir}\") \n",
    "\n",
    "    all_scores = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test,y_test[:specs.init_window])\n",
    "        score = specs.metric_func(y_test, y_pred)\n",
    "        all_scores.append(score)\n",
    "            \n",
    "    if not all_scores:\n",
    "        final_score = np.nan \n",
    "        print(f\"Warning: No valid scores calculated for benchmark {specs.name}.\")\n",
    "    else:\n",
    "        final_score = np.mean(all_scores).item() # Ensure scalar float\n",
    "\n",
    "    return {'metric_score': final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class BenchmarkSpecBase: pass \n",
    "class BenchmarkSpecBase:\n",
    "    \"\"\"\n",
    "    Base class for benchmark specifications, holding common attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name: str, # Unique name identifying this benchmark task.\n",
    "                 dataset_id: str, # Identifier for the raw dataset source.\n",
    "                 u_cols: List[str], # List of column names for input signals (u).\n",
    "                 y_cols: List[str], # List of column names for output signals (y).\n",
    "                 metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "                 x_cols: Optional[List[str]] = None, # Optional state inputs (x).\n",
    "                 sampling_time: Optional[float] = None, # Optional sampling time (seconds).\n",
    "                 download_func: Optional[Callable[[Path, bool], None]] = None, # Dataset preparation func.\n",
    "                 test_func: Callable[[BenchmarkSpecBase, Callable], Dict[str, Any]] = _test_simulation, # Evaluation func.\n",
    "                 init_window: Optional[int] = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "                 data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_func = test_func\n",
    "        self.init_window = init_window\n",
    "        self._data_root = data_root\n",
    "\n",
    "        # Ensure required parameters have valid values if needed (basic checks)\n",
    "        if not self.name or not self.dataset_id or not self.u_cols or not self.y_cols or not self.metric_func:\n",
    "             raise ValueError(\"Core benchmark parameters (name, dataset_id, u_cols, y_cols, metric_func) are required.\")\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        \"\"\"Returns the evaluated data root path.\"\"\"\n",
    "        if isinstance(self._data_root, Callable):\n",
    "            return self._data_root()\n",
    "        return self._data_root\n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the dataset directory.\"\"\"\n",
    "        return self.data_root / self.dataset_id\n",
    "\n",
    "    def ensure_dataset_exists(self, force_download: bool = False) -> None:\n",
    "        \"\"\"Checks if the dataset exists, downloads/prepares it if needed.\"\"\"\n",
    "        # (Implementation remains the same as before)\n",
    "        dataset_path = self.dataset_path\n",
    "        if self.download_func is None:\n",
    "            print(f\"Warning: No download function for '{self.name}'. Assuming data exists at {dataset_path}\")\n",
    "            if not dataset_path.is_dir():\n",
    "                 print(f\"Warning: Dataset directory {dataset_path} not found.\")\n",
    "            return\n",
    "\n",
    "        if not dataset_path.is_dir() or force_download:\n",
    "            print(f\"Preparing dataset for '{self.name}' at {dataset_path}...\")\n",
    "            self.data_root.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                self.download_func(dataset_path, force_download)\n",
    "                print(f\"Dataset '{self.name}' prepared successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing dataset '{self.name}': {e}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecSimulation(BenchmarkSpecBase):\n",
    "    \"\"\"\n",
    "    Specification for a simulation benchmark task.\n",
    "\n",
    "    Inherits common parameters from BaseBenchmarkSpec.\n",
    "    Use this when the goal is to simulate the system's output given the input `u`.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _test_prediction(specs: BenchmarkSpecBase, model: Callable):\n",
    "    test_dir = specs.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "\n",
    "    if not test_files:\n",
    "        raise RuntimeError(f\"No test files found in {test_dir}\") \n",
    "\n",
    "    all_scores = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test,y_test[:specs.init_window])\n",
    "        score = specs.metric_func(y_test, y_pred)\n",
    "        all_scores.append(score)\n",
    "            \n",
    "    if not all_scores:\n",
    "        final_score = np.nan \n",
    "        print(f\"Warning: No valid scores calculated for benchmark {specs.name}.\")\n",
    "    else:\n",
    "        final_score = np.mean(all_scores).item() # Ensure scalar float\n",
    "\n",
    "    return {'metric_score': final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecPrediction(BenchmarkSpecBase):\n",
    "    \"\"\"\n",
    "    Specification for a k-step ahead prediction benchmark task.\n",
    "\n",
    "    Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
    "    Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pred_horizon: int, # The 'k' in k-step ahead prediction (mandatory for this type).\n",
    "                 pred_step: int, # Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t).\n",
    "                 test_func: Callable[[BenchmarkSpecBase, Callable], Dict[str, Any]] = _test_prediction, # Evaluation func.\n",
    "                 **kwargs # Capture all base class arguments\n",
    "                ):\n",
    "        super().__init__(**kwargs) # Initialize base class attributes\n",
    "        if pred_horizon <= 0:\n",
    "             raise ValueError(\"pred_horizon must be a positive integer for PredictionBenchmarkSpec.\")\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.pred_step = pred_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/daniel-om-weber/identibench/blob/main/identibench/benchmark.py#L140){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BenchmarkSpecPrediction\n",
       "\n",
       ">      BenchmarkSpecPrediction (pred_horizon:int, pred_step:int, test_func:Calla\n",
       ">                               ble[[__main__.BenchmarkSpecBase,Callable],Dict[s\n",
       ">                               tr,Any]]=<function _test_prediction>, **kwargs)\n",
       "\n",
       "*Specification for a k-step ahead prediction benchmark task.\n",
       "\n",
       "Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
       "Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pred_horizon | int |  | The 'k' in k-step ahead prediction (mandatory for this type). |\n",
       "| pred_step | int |  | Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t). |\n",
       "| test_func | Callable | _test_prediction | Evaluation func. |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/daniel-om-weber/identibench/blob/main/identibench/benchmark.py#L140){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BenchmarkSpecPrediction\n",
       "\n",
       ">      BenchmarkSpecPrediction (pred_horizon:int, pred_step:int, test_func:Calla\n",
       ">                               ble[[__main__.BenchmarkSpecBase,Callable],Dict[s\n",
       ">                               tr,Any]]=<function _test_prediction>, **kwargs)\n",
       "\n",
       "*Specification for a k-step ahead prediction benchmark task.\n",
       "\n",
       "Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
       "Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pred_horizon | int |  | The 'k' in k-step ahead prediction (mandatory for this type). |\n",
       "| pred_step | int |  | Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t). |\n",
       "| test_func | Callable | _test_prediction | Evaluation func. |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nbdev import show_doc\n",
    "show_doc(BenchmarkSpecPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec basic initialization and defaults\n",
    "_spec_sim = BenchmarkSpecSimulation(\n",
    "    name='_spec_default', dataset_id='_dummy_default',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader\n",
    ")\n",
    "test_eq(_spec_sim.init_window, None)\n",
    "test_eq(_spec_sim.name, '_spec_default') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec initialization with prediction-related parameters\n",
    "_spec_pred = BenchmarkSpecPrediction(\n",
    "    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    init_window=20, pred_horizon=5, pred_step=2\n",
    ")\n",
    "test_eq(_spec_pred.init_window, 20)\n",
    "test_eq(_spec_pred.pred_horizon, 5)\n",
    "test_eq(_spec_pred.pred_step, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n",
    "_spec_ensure = BenchmarkSpecSimulation(\n",
    "    name='_spec_ensure', dataset_id='_dummy_ensure',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader\n",
    ")\n",
    "_spec_ensure.ensure_dataset_exists()\n",
    "_dataset_path_ensure = _spec_ensure.dataset_path\n",
    "test_eq(_dataset_path_ensure.is_dir(), True)\n",
    "test_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n",
    "_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists() \n",
    "_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_eq(_mtime_before_skip, _mtime_after_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for '_spec_ensure' at /Users/daniel/.identibench_data/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n",
    "_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists(force_download=True) \n",
    "_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_ne(_mtime_before_force, _mtime_after_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingContext:\n",
    "    \"\"\"\n",
    "    Context object passed to the user's training function (`build_predictor`).\n",
    "\n",
    "    Holds the benchmark specification, hyperparameters, and seed.\n",
    "    Provides methods to access the raw, full-length training and validation data sequences.\n",
    "    Windowing/batching for training must be handled within the user's `build_predictor` function.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 spec: BenchmarkSpecBase, # The benchmark specification.\n",
    "                 hyperparameters: Dict[str, Any], # User-provided dictionary containing model and training hyperparameters.\n",
    "                 seed: Optional[int] = None # Optional random seed for reproducibility.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.spec = spec\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.seed = seed\n",
    "\n",
    "    # --- Data Access Methods ---\n",
    "\n",
    "    def _get_file_paths(self, subset: str) -> List[Path]:\n",
    "        \"\"\"Gets sorted list of HDF5 files for a given subset directory.\"\"\"\n",
    "        subset_path = self.spec.dataset_path / subset\n",
    "        if not subset_path.is_dir():\n",
    "            return []\n",
    "        return sorted(list(subset_path.glob('*.hdf5')))\n",
    "\n",
    "    def _get_sequences_from_subset(self, subset: str\n",
    "                                  ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Loads raw sequences for a specific subset directory.\"\"\"\n",
    "        file_paths = self._get_file_paths(subset)\n",
    "        if not file_paths:\n",
    "             print(f\"Warning: No HDF5 files found in {self.spec.dataset_path / subset}. Returning empty iterator.\")\n",
    "             return iter([])\n",
    "\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=file_paths,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_train_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'train' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('train')\n",
    "\n",
    "    def get_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'valid' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('valid')\n",
    "\n",
    "    def get_train_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding raw (u, y, x) tuples for combined training and validation.\n",
    "\n",
    "        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.\n",
    "        If not, it loads data from 'train' and 'valid' subsets sequentially.\n",
    "        \"\"\"\n",
    "        train_valid_files = self._get_file_paths('train_valid')\n",
    "        if train_valid_files:\n",
    "            return _load_sequences_from_files(\n",
    "                file_paths=train_valid_files, u_cols=self.spec.u_cols, y_cols=self.spec.y_cols,\n",
    "                x_cols=self.spec.x_cols\n",
    "            )\n",
    "        else:\n",
    "            train_iter = self._get_sequences_from_subset('train')\n",
    "            valid_iter = self._get_sequences_from_subset('valid')\n",
    "            return itertools.chain(train_iter, valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_benchmark(spec, build_model, hyperparameters={}, seed=None):\n",
    "\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    results = {\n",
    "        'benchmark_name': spec.name,\n",
    "        'dataset_id': spec.dataset_id,\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'seed': seed,\n",
    "        'training_time_seconds': np.nan,\n",
    "        'test_time_seconds': np.nan,\n",
    "        'benchmark_type' : type(spec).__name__\n",
    "    }\n",
    "\n",
    "    spec.ensure_dataset_exists() \n",
    "\n",
    "    context = TrainingContext(spec=spec, hyperparameters=hyperparameters, seed=seed) \n",
    "\n",
    "    train_start_time = time.monotonic()\n",
    "    model = build_model(context) \n",
    "    train_end_time = time.monotonic()\n",
    "    results['training_time_seconds'] = train_end_time - train_start_time\n",
    "\n",
    "    if model is None:\n",
    "        raise RuntimeError(f\"build_model for {spec.name} did not return a model.\") \n",
    "        \n",
    "    test_start_time = time.monotonic()\n",
    "    test_results = spec.test_func(spec, model)\n",
    "    test_end_time = time.monotonic()\n",
    "\n",
    "    results['test_time_seconds'] = test_end_time - test_start_time\n",
    "    \n",
    "    results.update(test_results) # Merge test results\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Define a very simple build_model function for the example\n",
    "def _dummy_build_model(context):\n",
    "    print(f\"Building model with spec: {context.spec.name}, seed: {context.seed}\")\n",
    "\n",
    "    def dummy_model(u_test,y_test):\n",
    "        output_dim = len(context.spec.y_cols) \n",
    "        return np.zeros((u_test.shape[0], output_dim))\n",
    "        \n",
    "    return dummy_model # Return the callable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: _spec_default, seed: 2196568183\n",
      "\n",
      "Benchmark Results:\n",
      "{'benchmark_name': '_spec_default', 'dataset_id': '_dummy_default', 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5}, 'seed': 2196568183, 'training_time_seconds': 1.8916005501523614e-05, 'test_time_seconds': 0.001395874991430901, 'benchmark_type': 'BenchmarkSpecSimulation', 'metric_score': 0.5947432613358903}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of run_benchmark\n",
    "hyperparams = {'learning_rate': 0.01, 'epochs': 5} # Example hyperparameters\n",
    "\n",
    "results = run_benchmark(\n",
    "    spec=_spec_sim, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_test_logic(spec, model):\n",
    "    test_dir = spec.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "    max_errors = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, spec.u_cols, spec.y_cols, spec.x_cols):\n",
    "        y_pred = model(u_test,y_test[:spec.init_window])\n",
    "        max_errors.append(np.max(np.abs(y_test - y_pred)))\n",
    "\n",
    "    avg_max_error = np.mean(max_errors) if max_errors else np.nan\n",
    "    median_max_error = np.median(max_errors) if max_errors else np.nan\n",
    "    return {'avg_max_abs_error': avg_max_error, 'median_max_abs_error': median_max_error} # Return results as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_with_custom_test = BenchmarkSpecSimulation(\n",
    "    name=\"CustomTestExampleBench\",\n",
    "    dataset_id=\"dummy_core_data_v1\", # Same dataset ID as before\n",
    "    download_func=_dummy_dataset_loader, \n",
    "    u_cols=['u0', 'u1'], \n",
    "    y_cols=['y0'],      \n",
    "    test_func=custom_test_logic,\n",
    "    metric_func=identibench.metrics.rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: CustomTestExampleBench, seed: 218001267\n",
      "\n",
      "Benchmark Results (Custom Test Example):\n",
      "{'benchmark_name': 'CustomTestExampleBench', 'dataset_id': 'dummy_core_data_v1', 'hyperparameters': {'model_type': 'dummy_v2'}, 'seed': 218001267, 'training_time_seconds': 8.095901284832507e-05, 'test_time_seconds': 0.0012364580034045503, 'benchmark_type': 'BenchmarkSpecSimulation', 'avg_max_abs_error': np.float64(0.9871239066123962), 'median_max_abs_error': np.float64(0.9871239066123962)}\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark using the spec with the custom test function\n",
    "hyperparams = {'model_type': 'dummy_v2'} \n",
    "\n",
    "results_custom_test = run_benchmark(\n",
    "    spec=spec_with_custom_test, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmark Results (Custom Test Example):\")\n",
    "print(results_custom_test)\n",
    "\n",
    "# Note: The result dictionary now contains 'avg_max_abs_error' instead of 'metric_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_multiple_benchmarks(\n",
    "    specs: Union[List[BenchmarkSpecBase], Dict[str, BenchmarkSpecBase]], # Collection of specs to run\n",
    "    build_model: Callable[[TrainingContext], Callable], # User function to build the model/predictor\n",
    "    hyperparameters: Optional[Dict[str, Any]] = None, # Hyperparameters passed to build_model\n",
    "    seed: Optional[int] = None, # Base random seed\n",
    "    continue_on_error: bool = True, # If True, continue running benchmarks even if one fails\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs multiple benchmarks sequentially using the same build_model function.\n",
    "\n",
    "    Args:\n",
    "        specs: A list or dictionary containing the BenchmarkSpec objects to run.\n",
    "        build_model: A callable that accepts a TrainingContext and returns a trained model/predictor function.\n",
    "        hyperparameters: A dictionary of hyperparameters passed to the build_model function.\n",
    "        seed: An optional integer seed passed to each run_benchmark call for reproducibility.\n",
    "        continue_on_error: If True, catches exceptions during individual benchmark runs, prints a warning,\n",
    "                           and continues. If False, stops on the first error.\n",
    "\n",
    "    Returns:\n",
    "        A list of result dictionaries containing the results from successful benchmark runs.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    hyperparameters = hyperparameters or {} # Ensure it's a dict\n",
    "\n",
    "    # Determine the list of specification objects to iterate over\n",
    "    spec_objects = list(specs.values()) if isinstance(specs, dict) else list(specs)\n",
    "\n",
    "    print(f\"--- Starting benchmark run for {len(spec_objects)} specifications ---\")\n",
    "\n",
    "    for i, spec in enumerate(spec_objects):\n",
    "        spec_name = getattr(spec, 'name', f'Unnamed Spec {i+1}') # Get name for logging\n",
    "        print(f\"\\n[{i+1}/{len(spec_objects)}] Running benchmark: {spec_name}\")\n",
    "\n",
    "        try:\n",
    "            # Run the individual benchmark\n",
    "            result = run_benchmark(\n",
    "                spec=spec,\n",
    "                build_model=build_model,\n",
    "                hyperparameters=hyperparameters,\n",
    "                seed=seed # Pass the same base seed to each run\n",
    "            )\n",
    "            results_list.append(result)\n",
    "            print(f\"  -> Success: {spec_name} completed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERROR running benchmark '{spec_name}': {e}\")\n",
    "            if not continue_on_error:\n",
    "                print(\"Stopping due to error (continue_on_error=False).\")\n",
    "                raise # Re-raise the exception to halt execution\n",
    "            # If continue_on_error is True, the loop continues automatically\n",
    "\n",
    "    print(f\"\\n--- Benchmark run finished. {len(results_list)}/{len(spec_objects)} completed successfully. ---\")\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting benchmark run for 3 specifications ---\n",
      "\n",
      "[1/3] Running benchmark: _spec_default\n",
      "Building model with spec: _spec_default, seed: 4176054161\n",
      "  -> Success: _spec_default completed.\n",
      "\n",
      "[2/3] Running benchmark: _spec_pred_params\n",
      "Building model with spec: _spec_pred_params, seed: 1541742625\n",
      "  -> Success: _spec_pred_params completed.\n",
      "\n",
      "[3/3] Running benchmark: CustomTestExampleBench\n",
      "Building model with spec: CustomTestExampleBench, seed: 1357892571\n",
      "  -> Success: CustomTestExampleBench completed.\n",
      "\n",
      "--- Benchmark run finished. 3/3 completed successfully. ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'benchmark_name': '_spec_default',\n",
       "  'dataset_id': '_dummy_default',\n",
       "  'hyperparameters': {},\n",
       "  'seed': 4176054161,\n",
       "  'training_time_seconds': 1.3541997759602964e-05,\n",
       "  'test_time_seconds': 0.0019185830024071038,\n",
       "  'benchmark_type': 'BenchmarkSpecSimulation',\n",
       "  'metric_score': 0.5947432613358903},\n",
       " {'benchmark_name': '_spec_pred_params',\n",
       "  'dataset_id': '_dummy_pred_params',\n",
       "  'hyperparameters': {},\n",
       "  'seed': 1541742625,\n",
       "  'training_time_seconds': 5.750000127591193e-06,\n",
       "  'test_time_seconds': 0.0007422499911626801,\n",
       "  'benchmark_type': 'BenchmarkSpecPrediction',\n",
       "  'metric_score': 0.5681130975059453},\n",
       " {'benchmark_name': 'CustomTestExampleBench',\n",
       "  'dataset_id': 'dummy_core_data_v1',\n",
       "  'hyperparameters': {},\n",
       "  'seed': 1357892571,\n",
       "  'training_time_seconds': 5.832989700138569e-06,\n",
       "  'test_time_seconds': 0.0006590000120922923,\n",
       "  'benchmark_type': 'BenchmarkSpecSimulation',\n",
       "  'avg_max_abs_error': np.float64(0.9871239066123962),\n",
       "  'median_max_abs_error': np.float64(0.9871239066123962)}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_multiple_benchmarks(\n",
    "    specs=[_spec_sim,_spec_pred,spec_with_custom_test], \n",
    "    build_model=_dummy_build_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
