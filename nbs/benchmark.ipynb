{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Optional, Callable, Dict, Any, Iterator, Tuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import identibench.metrics\n",
    "from identibench.utils import get_default_data_root,_load_sequences_from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "import time # For testing modification times\n",
    "from fastcore.test import test_eq, test_ne# Import nbdev testing functions\n",
    "from identibench.utils import _dummy_dataset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Forward declaration needed for type hint in BenchmarkSpec.test_func\n",
    "class BenchmarkSpec: pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _test_simulation(specs, model):\n",
    "    test_dir = specs.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "\n",
    "    if not test_files:\n",
    "        raise RuntimeError(f\"No test files found in {test_dir}\") \n",
    "\n",
    "    all_scores = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test)\n",
    "        score = specs.metric_func(y_test, y_pred)\n",
    "        all_scores.append(score)\n",
    "            \n",
    "    if not all_scores:\n",
    "        final_score = np.nan \n",
    "        print(f\"Warning: No valid scores calculated for benchmark {specs.name}.\")\n",
    "    else:\n",
    "        final_score = np.mean(all_scores).item() # Ensure scalar float\n",
    "\n",
    "    return {'metric_score': final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _test_prediction(specs: BenchmarkSpec, model: Callable):\n",
    "    test_dir = specs.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "\n",
    "    if not test_files:\n",
    "        raise RuntimeError(f\"No test files found in {test_dir}\") \n",
    "\n",
    "    all_scores = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test)\n",
    "        score = specs.metric_func(y_test, y_pred)\n",
    "        all_scores.append(score)\n",
    "            \n",
    "    if not all_scores:\n",
    "        final_score = np.nan \n",
    "        print(f\"Warning: No valid scores calculated for benchmark {specs.name}.\")\n",
    "    else:\n",
    "        final_score = np.mean(all_scores).item() # Ensure scalar float\n",
    "\n",
    "    return {'metric_score': final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpec:\n",
    "    \"\"\"\n",
    "    Specification for a single, standardized benchmark dataset configuration.\n",
    "\n",
    "    Defines fixed parameters for dataset loading, preprocessing, evaluation metric,\n",
    "    and potentially a custom testing function. Specific evaluation logic \n",
    "    (simulation vs prediction, windowing) is handled by the benchmark execution function \n",
    "    or the custom test_func.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 name: str, # Unique name identifying this specific benchmark task (e.g., 'silverbox_sim_rmse').\n",
    "                 dataset_id: str, # Identifier for the raw dataset source (e.g., 'silverbox'), corresponds to subdirectory name.\n",
    "                 u_cols: List[str], # List of column names for the input signals (u).\n",
    "                 y_cols: List[str], # List of column names for the output signals (y).\n",
    "                 metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric for final test evaluation. `func(y_true, y_pred)`.\n",
    "                 x_cols: Optional[List[str]] = None, # Optional list of column names for state inputs (x).\n",
    "                 sampling_time: Optional[float] = None, # Optional sampling time (in seconds) if constant for the dataset.\n",
    "                 download_func: Optional[Callable[[Path, bool], None]] = None, # Function to download/prepare the raw dataset. `func(save_path, force_download)`\n",
    "                 test_sim_func: Optional[Callable[[BenchmarkSpec, Callable[[np.ndarray], np.ndarray]], Dict[str, Any]]] = _test_simulation, #\n",
    "                 test_pred_func: Optional[Callable[[BenchmarkSpec, Callable[[np.ndarray], np.ndarray]], Dict[str, Any]]] = _test_prediction, #\n",
    "                 init_window: Optional[int] = None, # Number of initial steps potentially used for model initialization (simulation or prediction).\n",
    "                 pred_horizon: Optional[int] = None, # The 'k' in k-step ahead prediction, used if the benchmark function performs prediction.\n",
    "                 pred_step: int = 1, # Step size for k-step ahead prediction, used if the benchmark function performs prediction.\n",
    "                 data_root_func: Callable[[], Path] = get_default_data_root # Function that returns the root directory where datasets are stored.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func # Now mandatory\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_sim_func = test_sim_func\n",
    "        self.test_pred_func = test_pred_func\n",
    "        self.init_window = init_window\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.pred_step = pred_step\n",
    "        self.data_root_func = data_root_func\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        \"\"\"Returns the evaluated data root path.\"\"\"\n",
    "        return self.data_root_func() \n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the dataset directory.\"\"\"\n",
    "        return self.data_root / self.dataset_id\n",
    "\n",
    "    def ensure_dataset_exists(self, force_download: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Checks if the dataset exists locally, downloads it if not or if forced.\n",
    "\n",
    "        Args:\n",
    "            force_download: If True, download the dataset even if it exists locally.\n",
    "        \"\"\"\n",
    "        dataset_path = self.dataset_path \n",
    "        download_func = self.download_func \n",
    "        if download_func is None:\n",
    "            print(f\"Warning: No download function specified for benchmark '{self.name}'. Cannot ensure data exists at {dataset_path}\")\n",
    "            # Check existence even if we can't download\n",
    "            if not dataset_path.is_dir():\n",
    "                 print(f\"Warning: Dataset directory {dataset_path} not found.\")\n",
    "            return\n",
    "\n",
    "        dataset_exists = dataset_path.is_dir()\n",
    "\n",
    "        if not dataset_exists or force_download:\n",
    "            print(f\"Dataset for '{self.name}' {'not found' if not dataset_exists else 'download forced'}. Preparing dataset at {dataset_path}...\")\n",
    "            self.data_root.mkdir(parents=True, exist_ok=True) \n",
    "            try:\n",
    "                download_func(dataset_path, force_download) \n",
    "                print(f\"Dataset '{self.name}' prepared successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing dataset '{self.name}': {e}\")\n",
    "                raise\n",
    "        else:\n",
    "             # Optionally print message if dataset already exists and not forced\n",
    "             # print(f\"Dataset for '{self.name}' found at {dataset_path}.\")\n",
    "             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup shared for BenchmarkSpec Tests\n",
    "_test_data_dir_spec = Path('./_temp_identibench_data_spec_test')\n",
    "shutil.rmtree(_test_data_dir_spec, ignore_errors=True) # Clean before tests\n",
    "def _get_test_data_root_spec(): return _test_data_dir_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec basic initialization and defaults\n",
    "_spec_default = BenchmarkSpec(\n",
    "    name='_spec_default', dataset_id='_dummy_default',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_default.init_window, None)\n",
    "test_eq(_spec_default.pred_horizon, None)\n",
    "test_eq(_spec_default.pred_step, 1)\n",
    "test_eq(_spec_default.name, '_spec_default') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec initialization with prediction-related parameters\n",
    "_spec_pred_params = BenchmarkSpec(\n",
    "    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    init_window=20, pred_horizon=5, pred_step=2,\n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_pred_params.init_window, 20)\n",
    "test_eq(_spec_pred_params.pred_horizon, 5)\n",
    "test_eq(_spec_pred_params.pred_step, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_spec_ensure' not found. Preparing dataset at _temp_identibench_data_spec_test/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n",
    "_spec_ensure = BenchmarkSpec(\n",
    "    name='_spec_ensure', dataset_id='_dummy_ensure',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "_spec_ensure.ensure_dataset_exists()\n",
    "_dataset_path_ensure = _spec_ensure.dataset_path\n",
    "test_eq(_dataset_path_ensure.is_dir(), True)\n",
    "test_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n",
    "_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists() \n",
    "_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_eq(_mtime_before_skip, _mtime_after_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_spec_ensure' download forced. Preparing dataset at _temp_identibench_data_spec_test/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n",
    "_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists(force_download=True) \n",
    "_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_ne(_mtime_before_force, _mtime_after_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingContext:\n",
    "    \"\"\"\n",
    "    Context object passed to the user's training function (`build_predictor`).\n",
    "\n",
    "    Holds the benchmark specification, hyperparameters, and seed.\n",
    "    Provides methods to access the raw, full-length training and validation data sequences.\n",
    "    Windowing/batching for training must be handled within the user's `build_predictor` function.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 spec: BenchmarkSpec, # The benchmark specification.\n",
    "                 hyperparameters: Dict[str, Any], # User-provided dictionary containing model and training hyperparameters.\n",
    "                 seed: Optional[int] = None # Optional random seed for reproducibility.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.spec = spec\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.seed = seed\n",
    "\n",
    "    # --- Data Access Methods ---\n",
    "\n",
    "    def _get_file_paths(self, subset: str) -> List[Path]:\n",
    "        \"\"\"Gets sorted list of HDF5 files for a given subset directory.\"\"\"\n",
    "        subset_path = self.spec.dataset_path / subset\n",
    "        if not subset_path.is_dir():\n",
    "            return []\n",
    "        return sorted(list(subset_path.glob('*.hdf5')))\n",
    "\n",
    "    def _get_sequences_from_subset(self, subset: str\n",
    "                                  ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Loads raw sequences for a specific subset directory.\"\"\"\n",
    "        file_paths = self._get_file_paths(subset)\n",
    "        if not file_paths:\n",
    "             print(f\"Warning: No HDF5 files found in {self.spec.dataset_path / subset}. Returning empty iterator.\")\n",
    "             return iter([])\n",
    "\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=file_paths,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_train_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'train' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('train')\n",
    "\n",
    "    def get_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'valid' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('valid')\n",
    "\n",
    "    def get_train_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding raw (u, y, x) tuples for combined training and validation.\n",
    "\n",
    "        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.\n",
    "        If not, it loads data from 'train' and 'valid' subsets sequentially.\n",
    "        \"\"\"\n",
    "        train_valid_files = self._get_file_paths('train_valid')\n",
    "        if train_valid_files:\n",
    "            return _load_sequences_from_files(\n",
    "                file_paths=train_valid_files, u_cols=self.spec.u_cols, y_cols=self.spec.y_cols,\n",
    "                x_cols=self.spec.x_cols\n",
    "            )\n",
    "        else:\n",
    "            train_iter = self._get_sequences_from_subset('train')\n",
    "            valid_iter = self._get_sequences_from_subset('valid')\n",
    "            return itertools.chain(train_iter, valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup shared for TrainingContext Tests\n",
    "_test_data_dir_ctx = Path('./_temp_identibench_data_ctx_test')\n",
    "_test_data_dir_ctx_tv = Path('./_temp_identibench_data_ctx_tv_test') \n",
    "shutil.rmtree(_test_data_dir_ctx, ignore_errors=True)\n",
    "shutil.rmtree(_test_data_dir_ctx_tv, ignore_errors=True)\n",
    "def _get_test_data_root_ctx(): return _test_data_dir_ctx\n",
    "def _get_test_data_root_ctx_tv(): return _test_data_dir_ctx_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_dummy_ctx_base' not found. Preparing dataset at _temp_identibench_data_ctx_test/_dummy_dataset_ctx_base...\n",
      "Dataset '_dummy_ctx_base' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create base dummy data (no train_valid dir) \n",
    "_dummy_spec_ctx_base = BenchmarkSpec(\n",
    "    name='_dummy_ctx_base', dataset_id='_dummy_dataset_ctx_base',\n",
    "    u_cols=['u0', 'u1'], y_cols=['y0'],metric_func=identibench.metrics.rmse, \n",
    "    download_func=lambda p, f: _dummy_dataset_loader(p, force_download=f, create_train_valid_dir=False),\n",
    "    data_root_func=_get_test_data_root_ctx, init_window=10\n",
    ")\n",
    "_dummy_spec_ctx_base.ensure_dataset_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_dummy_ctx_tv' not found. Preparing dataset at _temp_identibench_data_ctx_tv_test/_dummy_dataset_ctx_tv...\n",
      "Dataset '_dummy_ctx_tv' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create dummy data WITH train_valid dir \n",
    "_dummy_spec_ctx_tv = BenchmarkSpec(\n",
    "    name='_dummy_ctx_tv', dataset_id='_dummy_dataset_ctx_tv',\n",
    "    u_cols=['u0', 'u1'], y_cols=['y0'],metric_func=identibench.metrics.rmse, \n",
    "    download_func=lambda p, f: _dummy_dataset_loader(p, force_download=f, create_train_valid_dir=True),\n",
    "    data_root_func=_get_test_data_root_ctx_tv, init_window=10\n",
    ")\n",
    "_dummy_spec_ctx_tv.ensure_dataset_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Shared constants for tests\n",
    "_seq_len_ctx = 50 \n",
    "_n_files_train_valid_ctx = 2 \n",
    "_n_files_tv_dir_ctx = 1 \n",
    "_hyperparams_ctx = {'lr': 0.01, 'hidden': 64}\n",
    "_seed_ctx = 42\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 16\n",
    "# Test: TrainingContext initialization\n",
    "_ctx = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "test_eq(_ctx.spec, _dummy_spec_ctx_base)\n",
    "test_eq(_ctx.hyperparameters, _hyperparams_ctx)\n",
    "test_eq(_ctx.seed, _seed_ctx)\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 17\n",
    "# Test: TrainingContext get_train_sequences\n",
    "_ctx = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_train_sequences = list(_ctx.get_train_sequences())\n",
    "test_eq(len(_train_sequences), _n_files_train_valid_ctx) \n",
    "_u_train, _y_train, _x_train = _train_sequences[0]\n",
    "test_eq(_u_train.shape, (_seq_len_ctx, len(_dummy_spec_ctx_base.u_cols))) \n",
    "test_eq(_y_train.shape, (_seq_len_ctx, len(_dummy_spec_ctx_base.y_cols)))\n",
    "test_eq(_x_train, None)\n",
    "test_eq(_u_train.dtype, np.float32)\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 18\n",
    "# Test: TrainingContext get_valid_sequences\n",
    "_ctx = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_valid_sequences = list(_ctx.get_valid_sequences())\n",
    "test_eq(len(_valid_sequences), _n_files_train_valid_ctx)\n",
    "_u_valid, _y_valid, _x_valid = _valid_sequences[0]\n",
    "test_eq(_u_valid.shape, (_seq_len_ctx, len(_dummy_spec_ctx_base.u_cols)))\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 19\n",
    "# Test: TrainingContext get_train_valid_sequences - fallback (no train_valid dir)\n",
    "_ctx_tv_fallback = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_tv_sequences_fallback = list(_ctx_tv_fallback.get_train_valid_sequences())\n",
    "test_eq(len(_tv_sequences_fallback), _n_files_train_valid_ctx + _n_files_train_valid_ctx)\n",
    "_u_tv_fb_train, _y_tv_fb_train, _ = _tv_sequences_fallback[0] \n",
    "test_eq(_u_tv_fb_train.shape[0], _seq_len_ctx)\n",
    "_u_tv_fb_valid, _y_tv_fb_valid, _ = _tv_sequences_fallback[_n_files_train_valid_ctx] \n",
    "test_eq(_u_tv_fb_valid.shape[0], _seq_len_ctx)\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 20\n",
    "# Test: TrainingContext get_train_valid_sequences - direct (train_valid dir exists)\n",
    "_ctx_tv_direct = TrainingContext(spec=_dummy_spec_ctx_tv, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_tv_sequences_direct = list(_ctx_tv_direct.get_train_valid_sequences())\n",
    "test_eq(len(_tv_sequences_direct), _n_files_tv_dir_ctx)\n",
    "_u_tv_direct, _y_tv_direct, _ = _tv_sequences_direct[0]\n",
    "test_eq(_u_tv_direct.shape[0], _seq_len_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_benchmark(spec, build_model, hyperparameters={}, seed=None):\n",
    "\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    results = {\n",
    "        'benchmark_name': spec.name,\n",
    "        'dataset_id': spec.dataset_id,\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'seed': seed,\n",
    "        'training_time_seconds': np.nan,\n",
    "        'test_time_seconds': np.nan,\n",
    "    }\n",
    "\n",
    "    spec.ensure_dataset_exists() \n",
    "\n",
    "    context = TrainingContext(spec=spec, hyperparameters=hyperparameters, seed=seed) \n",
    "\n",
    "    train_start_time = time.monotonic()\n",
    "    model = build_model(context) \n",
    "    train_end_time = time.monotonic()\n",
    "    results['training_time_seconds'] = train_end_time - train_start_time\n",
    "\n",
    "    if model is None:\n",
    "        raise RuntimeError(f\"build_model for {spec.name} did not return a model.\") \n",
    "        \n",
    "    test_start_time = time.monotonic()\n",
    "    test_results = spec.test_sim_func(spec, model)\n",
    "    test_end_time = time.monotonic()\n",
    "\n",
    "    results['test_time_seconds'] = test_end_time - test_start_time\n",
    "    \n",
    "    results.update(test_results) # Merge test results\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BenchmarkSpec matching the dummy data columns\n",
    "example_spec = BenchmarkSpec(\n",
    "    name=\"DummyLoaderExampleBench\",\n",
    "    dataset_id=\"dummy_core_data_v1\", \n",
    "    download_func=_dummy_dataset_loader,\n",
    "    u_cols=['u0', 'u1'],\n",
    "    y_cols=['y0'], \n",
    "    metric_func=identibench.metrics.rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Define a very simple build_model function for the example\n",
    "def simple_build_model(context):\n",
    "    print(f\"Building model with spec: {context.spec.name}, seed: {context.seed}\")\n",
    "\n",
    "    def dummy_model(u_test):\n",
    "        output_dim = len(context.spec.y_cols) \n",
    "        return np.zeros((u_test.shape[0], output_dim))\n",
    "        \n",
    "    return dummy_model # Return the callable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: DummyLoaderExampleBench, seed: 3475062729\n",
      "\n",
      "Benchmark Results:\n",
      "{'benchmark_name': 'DummyLoaderExampleBench', 'dataset_id': 'dummy_core_data_v1', 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5}, 'seed': 3475062729, 'training_time_seconds': 0.00010208300955127925, 'test_time_seconds': 0.0012477500131353736, 'metric_score': 0.5632057005902633}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of run_benchmark\n",
    "hyperparams = {'learning_rate': 0.01, 'epochs': 5} # Example hyperparameters\n",
    "\n",
    "results = run_benchmark(\n",
    "    spec=example_spec, \n",
    "    build_model=simple_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_test_logic(spec, model):\n",
    "    test_dir = spec.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "    max_errors = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, spec.u_cols, spec.y_cols, spec.x_cols):\n",
    "        y_pred = model(u_test)\n",
    "        max_errors.append(np.max(np.abs(y_test - y_pred)))\n",
    "\n",
    "    avg_max_error = np.mean(max_errors) if max_errors else np.nan\n",
    "    median_max_error = np.median(max_errors) if max_errors else np.nan\n",
    "    return {'avg_max_abs_error': avg_max_error, 'median_max_abs_error': median_max_error} # Return results as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_with_custom_test = BenchmarkSpec(\n",
    "    name=\"CustomTestExampleBench\",\n",
    "    dataset_id=\"dummy_core_data_v1\", # Same dataset ID as before\n",
    "    download_func=_dummy_dataset_loader, \n",
    "    u_cols=['u0', 'u1'], \n",
    "    y_cols=['y0'],      \n",
    "    test_sim_func=custom_test_logic,\n",
    "    metric_func=identibench.metrics.rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: CustomTestExampleBench, seed: 2049681544\n",
      "\n",
      "Benchmark Results (Custom Test Example):\n",
      "{'benchmark_name': 'CustomTestExampleBench', 'dataset_id': 'dummy_core_data_v1', 'hyperparameters': {'model_type': 'dummy_v2'}, 'seed': 2049681544, 'training_time_seconds': 0.00013220800610724837, 'test_time_seconds': 0.0012791250046575442, 'avg_max_abs_error': np.float64(0.9878208935260773), 'median_max_abs_error': np.float64(0.9878208935260773)}\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark using the spec with the custom test function\n",
    "hyperparams = {'model_type': 'dummy_v2'} \n",
    "\n",
    "results_custom_test = run_benchmark(\n",
    "    spec=spec_with_custom_test, \n",
    "    build_model=simple_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmark Results (Custom Test Example):\")\n",
    "print(results_custom_test)\n",
    "\n",
    "# Note: The result dictionary now contains 'avg_max_abs_error' instead of 'metric_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
