{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "import numpy as np\n",
    "from identibench.core import BenchmarkSpec, TrainingContext, _load_raw_sequences_from_files\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "from fastcore.test import test_eq, test_ne# Import nbdev testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _run_default_test(\n",
    "    spec: BenchmarkSpec, # The benchmark specification.\n",
    "    model: Callable[[np.ndarray], np.ndarray] # The trained model (assumed callable for simulation).\n",
    "    ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs the default testing logic (simulation) using the spec's metric_func.\n",
    "    \n",
    "    Args:\n",
    "        spec: The BenchmarkSpec.\n",
    "        model: The trained model, expected to accept u_test and return y_pred.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the calculated metric score (e.g., {'metric_score': 0.123}).\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If no test files are found.\n",
    "        ValueError: If prediction shape mismatch occurs. \n",
    "        Exception: Propagates exceptions from model call or metric calculation.\n",
    "    \"\"\"\n",
    "    test_dir = spec.dataset_path / 'test'\n",
    "    # Use glob from the imported module\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "    \n",
    "    if not test_files:\n",
    "        # Let's raise an error here, as testing without test files is problematic\n",
    "        raise RuntimeError(f\"No test files found in {test_dir}\") \n",
    "\n",
    "    all_scores = []\n",
    "    for test_file in test_files:\n",
    "        # Load the single sequence from the file\n",
    "        # This now correctly calls the _load_raw_sequences_from_files defined above\n",
    "        u_test, y_test, _ = next(_load_raw_sequences_from_files([test_file], spec.u_cols, spec.y_cols, spec.x_cols))\n",
    "        \n",
    "        # Run prediction\n",
    "        y_pred = model(u_test)\n",
    "\n",
    "        # Validate prediction shape\n",
    "        if y_pred.shape != y_test.shape:\n",
    "            # Use a standard ValueError\n",
    "            raise ValueError(f\"Prediction shape mismatch in {test_file.name}. Expected {y_test.shape}, got {y_pred.shape}\")\n",
    "\n",
    "        # Calculate metric (allow metric_func errors to propagate)\n",
    "        score = spec.metric_func(y_test, y_pred)\n",
    "        if not isinstance(score, (float, np.number)):\n",
    "             print(f\"Warning: Metric function for {spec.name} did not return a number (got {type(score)}). Attempting conversion.\")\n",
    "             try: score = float(score)\n",
    "             except Exception as e: \n",
    "                 print(f\"Warning: Could not convert metric score to float: {e}. Using NaN.\")\n",
    "                 score = np.nan\n",
    "        all_scores.append(score)\n",
    "            \n",
    "    # Calculate average score\n",
    "    if not all_scores:\n",
    "        final_score = np.nan \n",
    "        print(f\"Warning: No valid scores calculated for benchmark {spec.name}.\")\n",
    "    else:\n",
    "        final_score = np.mean(all_scores).item() # Ensure scalar float\n",
    "\n",
    "    return {'metric_score': final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_benchmark(\n",
    "    spec: BenchmarkSpec, # The benchmark specification object.\n",
    "    build_model: Callable[[TrainingContext], Callable], # User function to build and train the model.\n",
    "    hyperparameters: Dict[str, Any], # Dictionary of hyperparameters for this run.\n",
    "    seed: Optional[int] = None, # Optional random seed for reproducibility. If None, a random seed is generated.\n",
    "    ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs a single benchmark configuration.\n",
    "\n",
    "    Handles dataset preparation, model training, testing (default simulation or custom),\n",
    "    timing, and result aggregation. Allows exceptions to propagate.\n",
    "\n",
    "    Args:\n",
    "        spec: The BenchmarkSpec defining the benchmark.\n",
    "        build_model: A function that takes a TrainingContext and returns a trained, callable model.\n",
    "        hyperparameters: A dictionary containing hyperparameters for the build_model function.\n",
    "        seed: An optional integer seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the results (metrics, timings, config).\n",
    "\n",
    "    Raises:\n",
    "        Exception: Propagates errors from dataset preparation, training, or testing phases.\n",
    "    \"\"\"\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    # 1. Initialization & Setup\n",
    "    actual_seed = seed if seed is not None else random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    results = {\n",
    "        'benchmark_name': spec.name,\n",
    "        'dataset_id': spec.dataset_id,\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'seed': actual_seed,\n",
    "        'training_time_seconds': np.nan,\n",
    "        'test_time_seconds': np.nan,\n",
    "    }\n",
    "\n",
    "    # 2. Ensure Dataset (Allow exceptions to propagate)\n",
    "    spec.ensure_dataset_exists() \n",
    "\n",
    "    # 3. Create Context\n",
    "    # If core.py was separate, you'd import TrainingContext\n",
    "    context = TrainingContext(spec=spec, hyperparameters=hyperparameters, seed=actual_seed) \n",
    "\n",
    "    # 4. Training Phase (Allow exceptions to propagate)\n",
    "    train_start_time = time.monotonic()\n",
    "    model = build_model(context) \n",
    "    train_end_time = time.monotonic()\n",
    "    results['training_time_seconds'] = train_end_time - train_start_time\n",
    "\n",
    "    if model is None:\n",
    "        # Raise a standard error if model building fails silently\n",
    "        raise RuntimeError(f\"build_model for {spec.name} did not return a model.\") \n",
    "        \n",
    "    # 5. Testing & Evaluation Phase (Allow exceptions to propagate)\n",
    "    test_start_time = time.monotonic()\n",
    "    if spec.test_func is not None:\n",
    "        # Use custom test function\n",
    "        test_results = spec.test_func(spec, model)\n",
    "        if not isinstance(test_results, dict):\n",
    "             # Use standard error\n",
    "             raise TypeError(f\"Custom test_func for {spec.name} did not return a dictionary.\") \n",
    "    else:\n",
    "        # Use default test logic (simulation)\n",
    "        test_results = _run_default_test(spec, model)\n",
    "        \n",
    "    test_end_time = time.monotonic()\n",
    "    results['test_time_seconds'] = test_end_time - test_start_time\n",
    "    \n",
    "    # Merge test results into main results\n",
    "    results.update(test_results) \n",
    "        \n",
    "    # 6. Return successful results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
