{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Optional, Callable, Dict, Any, Iterator, Tuple\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "from identibench.utils import get_default_data_root,_load_sequences_from_files\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "from fastcore.test import test_eq, test_ne# Import nbdev testing functions\n",
    "import identibench.metrics\n",
    "from identibench.utils import _dummy_dataset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _test_simulation(specs, model):\n",
    "    test_dir = specs.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "\n",
    "    if not test_files:\n",
    "        raise RuntimeError(f\"No test files found in {test_dir}\") \n",
    "\n",
    "    all_scores = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test,y_test[:specs.init_window])\n",
    "        score = specs.metric_func(y_test, y_pred)\n",
    "        all_scores.append(score)\n",
    "            \n",
    "    if not all_scores:\n",
    "        final_score = np.nan \n",
    "        print(f\"Warning: No valid scores calculated for benchmark {specs.name}.\")\n",
    "    else:\n",
    "        final_score = np.mean(all_scores).item() # Ensure scalar float\n",
    "\n",
    "    return {'metric_score': final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class BenchmarkSpecBase: pass \n",
    "class BenchmarkSpecBase(abc.ABC): # Make it abstract if desired\n",
    "    \"\"\"\n",
    "    Base class for benchmark specifications, holding common attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name: str, # Unique name identifying this benchmark task.\n",
    "                 dataset_id: str, # Identifier for the raw dataset source.\n",
    "                 u_cols: List[str], # List of column names for input signals (u).\n",
    "                 y_cols: List[str], # List of column names for output signals (y).\n",
    "                 metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "                 x_cols: Optional[List[str]] = None, # Optional state inputs (x).\n",
    "                 sampling_time: Optional[float] = None, # Optional sampling time (seconds).\n",
    "                 download_func: Optional[Callable[[Path, bool], None]] = None, # Dataset preparation func.\n",
    "                 test_func: Callable[[BenchmarkSpecBase, Callable], Dict[str, Any]] = _test_simulation, # Evaluation func.\n",
    "                 init_window: Optional[int] = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "                 data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_func = test_func\n",
    "        self.init_window = init_window\n",
    "        self._data_root = data_root\n",
    "\n",
    "        # Ensure required parameters have valid values if needed (basic checks)\n",
    "        if not self.name or not self.dataset_id or not self.u_cols or not self.y_cols or not self.metric_func:\n",
    "             raise ValueError(\"Core benchmark parameters (name, dataset_id, u_cols, y_cols, metric_func) are required.\")\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        \"\"\"Returns the evaluated data root path.\"\"\"\n",
    "        if isinstance(self._data_root, Callable):\n",
    "            return self._data_root()\n",
    "        return self._data_root\n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the dataset directory.\"\"\"\n",
    "        return self.data_root / self.dataset_id\n",
    "\n",
    "    def ensure_dataset_exists(self, force_download: bool = False) -> None:\n",
    "        \"\"\"Checks if the dataset exists, downloads/prepares it if needed.\"\"\"\n",
    "        # (Implementation remains the same as before)\n",
    "        dataset_path = self.dataset_path\n",
    "        if self.download_func is None:\n",
    "            print(f\"Warning: No download function for '{self.name}'. Assuming data exists at {dataset_path}\")\n",
    "            if not dataset_path.is_dir():\n",
    "                 print(f\"Warning: Dataset directory {dataset_path} not found.\")\n",
    "            return\n",
    "\n",
    "        if not dataset_path.is_dir() or force_download:\n",
    "            print(f\"Preparing dataset for '{self.name}' at {dataset_path}...\")\n",
    "            self.data_root.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                self.download_func(dataset_path, force_download)\n",
    "                print(f\"Dataset '{self.name}' prepared successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing dataset '{self.name}': {e}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecSimulation(BenchmarkSpecBase):\n",
    "    \"\"\"\n",
    "    Specification for a simulation benchmark task.\n",
    "\n",
    "    Inherits common parameters from BaseBenchmarkSpec.\n",
    "    Use this when the goal is to simulate the system's output given the input `u`.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _test_prediction(specs: BenchmarkSpecBase, model: Callable):\n",
    "    test_dir = specs.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "\n",
    "    if not test_files:\n",
    "        raise RuntimeError(f\"No test files found in {test_dir}\") \n",
    "\n",
    "    all_scores = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test,y_test[:specs.init_window])\n",
    "        score = specs.metric_func(y_test, y_pred)\n",
    "        all_scores.append(score)\n",
    "            \n",
    "    if not all_scores:\n",
    "        final_score = np.nan \n",
    "        print(f\"Warning: No valid scores calculated for benchmark {specs.name}.\")\n",
    "    else:\n",
    "        final_score = np.mean(all_scores).item() # Ensure scalar float\n",
    "\n",
    "    return {'metric_score': final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecPrediction(BenchmarkSpecBase):\n",
    "    \"\"\"\n",
    "    Specification for a k-step ahead prediction benchmark task.\n",
    "\n",
    "    Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
    "    Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pred_horizon: int, # The 'k' in k-step ahead prediction (mandatory for this type).\n",
    "                 pred_step: int, # Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t).\n",
    "                 test_func: Callable[[BenchmarkSpecBase, Callable], Dict[str, Any]] = _test_prediction, # Evaluation func.\n",
    "                 **kwargs # Capture all base class arguments\n",
    "                ):\n",
    "        super().__init__(**kwargs) # Initialize base class attributes\n",
    "        if pred_horizon <= 0:\n",
    "             raise ValueError(\"pred_horizon must be a positive integer for PredictionBenchmarkSpec.\")\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.pred_step = pred_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/daniel-om-weber/identibench/blob/main/identibench/benchmark.py#L141){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BenchmarkSpecPrediction\n",
       "\n",
       ">      BenchmarkSpecPrediction (pred_horizon:int, pred_step:int, test_func:Calla\n",
       ">                               ble[[__main__.BenchmarkSpecBase,Callable],Dict[s\n",
       ">                               tr,Any]]=<function _test_prediction>, **kwargs)\n",
       "\n",
       "*Specification for a k-step ahead prediction benchmark task.\n",
       "\n",
       "Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
       "Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pred_horizon | int |  | The 'k' in k-step ahead prediction (mandatory for this type). |\n",
       "| pred_step | int |  | Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t). |\n",
       "| test_func | Callable | _test_prediction | Evaluation func. |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/daniel-om-weber/identibench/blob/main/identibench/benchmark.py#L141){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BenchmarkSpecPrediction\n",
       "\n",
       ">      BenchmarkSpecPrediction (pred_horizon:int, pred_step:int, test_func:Calla\n",
       ">                               ble[[__main__.BenchmarkSpecBase,Callable],Dict[s\n",
       ">                               tr,Any]]=<function _test_prediction>, **kwargs)\n",
       "\n",
       "*Specification for a k-step ahead prediction benchmark task.\n",
       "\n",
       "Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
       "Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pred_horizon | int |  | The 'k' in k-step ahead prediction (mandatory for this type). |\n",
       "| pred_step | int |  | Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t). |\n",
       "| test_func | Callable | _test_prediction | Evaluation func. |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nbdev import show_doc\n",
    "show_doc(BenchmarkSpecPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup shared for BenchmarkSpec Tests\n",
    "def _get_test_data_root_spec(): return Path('./tmp')\n",
    "shutil.rmtree(_get_test_data_root_spec(), ignore_errors=True) # Clean before tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec basic initialization and defaults\n",
    "_spec_sim = BenchmarkSpecSimulation(\n",
    "    name='_spec_default', dataset_id='_dummy_default',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    data_root=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_sim.init_window, None)\n",
    "test_eq(_spec_sim.name, '_spec_default') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec initialization with prediction-related parameters\n",
    "_spec_pred = BenchmarkSpecPrediction(\n",
    "    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    init_window=20, pred_horizon=5, pred_step=2,\n",
    "    data_root=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_pred.init_window, 20)\n",
    "test_eq(_spec_pred.pred_horizon, 5)\n",
    "test_eq(_spec_pred.pred_step, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for '_spec_ensure' at tmp/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n",
    "_spec_ensure = BenchmarkSpecSimulation(\n",
    "    name='_spec_ensure', dataset_id='_dummy_ensure',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    data_root=_get_test_data_root_spec\n",
    ")\n",
    "_spec_ensure.ensure_dataset_exists()\n",
    "_dataset_path_ensure = _spec_ensure.dataset_path\n",
    "test_eq(_dataset_path_ensure.is_dir(), True)\n",
    "test_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n",
    "_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists() \n",
    "_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_eq(_mtime_before_skip, _mtime_after_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for '_spec_ensure' at tmp/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n",
    "_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists(force_download=True) \n",
    "_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_ne(_mtime_before_force, _mtime_after_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingContext:\n",
    "    \"\"\"\n",
    "    Context object passed to the user's training function (`build_predictor`).\n",
    "\n",
    "    Holds the benchmark specification, hyperparameters, and seed.\n",
    "    Provides methods to access the raw, full-length training and validation data sequences.\n",
    "    Windowing/batching for training must be handled within the user's `build_predictor` function.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 spec: BenchmarkSpecBase, # The benchmark specification.\n",
    "                 hyperparameters: Dict[str, Any], # User-provided dictionary containing model and training hyperparameters.\n",
    "                 seed: Optional[int] = None # Optional random seed for reproducibility.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.spec = spec\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.seed = seed\n",
    "\n",
    "    # --- Data Access Methods ---\n",
    "\n",
    "    def _get_file_paths(self, subset: str) -> List[Path]:\n",
    "        \"\"\"Gets sorted list of HDF5 files for a given subset directory.\"\"\"\n",
    "        subset_path = self.spec.dataset_path / subset\n",
    "        if not subset_path.is_dir():\n",
    "            return []\n",
    "        return sorted(list(subset_path.glob('*.hdf5')))\n",
    "\n",
    "    def _get_sequences_from_subset(self, subset: str\n",
    "                                  ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Loads raw sequences for a specific subset directory.\"\"\"\n",
    "        file_paths = self._get_file_paths(subset)\n",
    "        if not file_paths:\n",
    "             print(f\"Warning: No HDF5 files found in {self.spec.dataset_path / subset}. Returning empty iterator.\")\n",
    "             return iter([])\n",
    "\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=file_paths,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_train_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'train' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('train')\n",
    "\n",
    "    def get_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'valid' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('valid')\n",
    "\n",
    "    def get_train_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding raw (u, y, x) tuples for combined training and validation.\n",
    "\n",
    "        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.\n",
    "        If not, it loads data from 'train' and 'valid' subsets sequentially.\n",
    "        \"\"\"\n",
    "        train_valid_files = self._get_file_paths('train_valid')\n",
    "        if train_valid_files:\n",
    "            return _load_sequences_from_files(\n",
    "                file_paths=train_valid_files, u_cols=self.spec.u_cols, y_cols=self.spec.y_cols,\n",
    "                x_cols=self.spec.x_cols\n",
    "            )\n",
    "        else:\n",
    "            train_iter = self._get_sequences_from_subset('train')\n",
    "            valid_iter = self._get_sequences_from_subset('valid')\n",
    "            return itertools.chain(train_iter, valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_benchmark(spec, build_model, hyperparameters={}, seed=None):\n",
    "\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    results = {\n",
    "        'benchmark_name': spec.name,\n",
    "        'dataset_id': spec.dataset_id,\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'seed': seed,\n",
    "        'training_time_seconds': np.nan,\n",
    "        'test_time_seconds': np.nan,\n",
    "        'benchmark_type' : type(spec).__name__\n",
    "    }\n",
    "\n",
    "    spec.ensure_dataset_exists() \n",
    "\n",
    "    context = TrainingContext(spec=spec, hyperparameters=hyperparameters, seed=seed) \n",
    "\n",
    "    train_start_time = time.monotonic()\n",
    "    model = build_model(context) \n",
    "    train_end_time = time.monotonic()\n",
    "    results['training_time_seconds'] = train_end_time - train_start_time\n",
    "\n",
    "    if model is None:\n",
    "        raise RuntimeError(f\"build_model for {spec.name} did not return a model.\") \n",
    "        \n",
    "    test_start_time = time.monotonic()\n",
    "    test_results = spec.test_func(spec, model)\n",
    "    test_end_time = time.monotonic()\n",
    "\n",
    "    results['test_time_seconds'] = test_end_time - test_start_time\n",
    "    \n",
    "    results.update(test_results) # Merge test results\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Define a very simple build_model function for the example\n",
    "def _dummy_build_model(context):\n",
    "    print(f\"Building model with spec: {context.spec.name}, seed: {context.seed}\")\n",
    "\n",
    "    def dummy_model(u_test,y_test):\n",
    "        output_dim = len(context.spec.y_cols) \n",
    "        return np.zeros((u_test.shape[0], output_dim))\n",
    "        \n",
    "    return dummy_model # Return the callable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for '_spec_default' at tmp/_dummy_default...\n",
      "Dataset '_spec_default' prepared successfully.\n",
      "Building model with spec: _spec_default, seed: 3200791986\n",
      "\n",
      "Benchmark Results:\n",
      "{'benchmark_name': '_spec_default', 'dataset_id': '_dummy_default', 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5}, 'seed': 3200791986, 'training_time_seconds': 2.9579969123005867e-06, 'test_time_seconds': 0.0006564170034835115, 'benchmark_type': 'BenchmarkSpecSimulation', 'metric_score': 0.5939152165770819}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of run_benchmark\n",
    "hyperparams = {'learning_rate': 0.01, 'epochs': 5} # Example hyperparameters\n",
    "\n",
    "results = run_benchmark(\n",
    "    spec=_spec_sim, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_test_logic(spec, model):\n",
    "    test_dir = spec.dataset_path / 'test'\n",
    "    test_files = sorted(list(test_dir.glob('*.hdf5'))) \n",
    "    max_errors = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(test_files, spec.u_cols, spec.y_cols, spec.x_cols):\n",
    "        y_pred = model(u_test,y_test[:spec.init_window])\n",
    "        max_errors.append(np.max(np.abs(y_test - y_pred)))\n",
    "\n",
    "    avg_max_error = np.mean(max_errors) if max_errors else np.nan\n",
    "    median_max_error = np.median(max_errors) if max_errors else np.nan\n",
    "    return {'avg_max_abs_error': avg_max_error, 'median_max_abs_error': median_max_error} # Return results as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_with_custom_test = BenchmarkSpecSimulation(\n",
    "    name=\"CustomTestExampleBench\",\n",
    "    dataset_id=\"dummy_core_data_v1\", # Same dataset ID as before\n",
    "    download_func=_dummy_dataset_loader, \n",
    "    u_cols=['u0', 'u1'], \n",
    "    y_cols=['y0'],      \n",
    "    test_func=custom_test_logic,\n",
    "    metric_func=identibench.metrics.rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: CustomTestExampleBench, seed: 3727038806\n",
      "\n",
      "Benchmark Results (Custom Test Example):\n",
      "{'benchmark_name': 'CustomTestExampleBench', 'dataset_id': 'dummy_core_data_v1', 'hyperparameters': {'model_type': 'dummy_v2'}, 'seed': 3727038806, 'training_time_seconds': 1.6457997844554484e-05, 'test_time_seconds': 0.0012796249939128757, 'benchmark_type': 'BenchmarkSpecSimulation', 'avg_max_abs_error': np.float64(0.9878208935260773), 'median_max_abs_error': np.float64(0.9878208935260773)}\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark using the spec with the custom test function\n",
    "hyperparams = {'model_type': 'dummy_v2'} \n",
    "\n",
    "results_custom_test = run_benchmark(\n",
    "    spec=spec_with_custom_test, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmark Results (Custom Test Example):\")\n",
    "print(results_custom_test)\n",
    "\n",
    "# Note: The result dictionary now contains 'avg_max_abs_error' instead of 'metric_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
