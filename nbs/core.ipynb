{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Optional, Callable, Dict, Any, Iterator, Tuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import itertools\n",
    "import identibench.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "import time # For testing modification times\n",
    "from fastcore.test import test_eq, test_ne# Import nbdev testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_default_data_root() -> Path:\n",
    "    \"\"\"\n",
    "    Returns the default root directory for datasets.\n",
    "\n",
    "    Checks the 'IDENTIBENCH_DATA_ROOT' environment variable first,\n",
    "    otherwise defaults to '~/.identibench_data'.\n",
    "    \"\"\"\n",
    "    return Path(os.environ.get('IDENTIBENCH_DATA_ROOT', Path.home() / '.identibench_data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Forward declaration needed for type hint in BenchmarkSpec.test_func\n",
    "class BenchmarkSpec: pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpec:\n",
    "    \"\"\"\n",
    "    Specification for a single, standardized benchmark dataset configuration.\n",
    "\n",
    "    Defines fixed parameters for dataset loading, preprocessing, evaluation metric,\n",
    "    and potentially a custom testing function. Specific evaluation logic \n",
    "    (simulation vs prediction, windowing) is handled by the benchmark execution function \n",
    "    or the custom test_func.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 name: str, # Unique name identifying this specific benchmark task (e.g., 'silverbox_sim_rmse').\n",
    "                 dataset_id: str, # Identifier for the raw dataset source (e.g., 'silverbox'), corresponds to subdirectory name.\n",
    "                 u_cols: List[str], # List of column names for the input signals (u).\n",
    "                 y_cols: List[str], # List of column names for the output signals (y).\n",
    "                 metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric for final test evaluation. `func(y_true, y_pred)`.\n",
    "                 x_cols: Optional[List[str]] = None, # Optional list of column names for state inputs (x).\n",
    "                 sampling_time: Optional[float] = None, # Optional sampling time (in seconds) if constant for the dataset.\n",
    "                 download_func: Optional[Callable[[Path, bool], None]] = None, # Function to download/prepare the raw dataset. `func(save_path, force_download)`\n",
    "                 test_func: Optional[Callable[[BenchmarkSpec, Callable[[np.ndarray], np.ndarray]], Dict[str, Any]]] = None, # Optional custom function to perform testing and return results dict. Takes spec and predictor. Overrides default test logic.\n",
    "                 init_window: Optional[int] = None, # Number of initial steps potentially used for model initialization (simulation or prediction).\n",
    "                 pred_horizon: Optional[int] = None, # The 'k' in k-step ahead prediction, used if the benchmark function performs prediction.\n",
    "                 pred_step: int = 1, # Step size for k-step ahead prediction, used if the benchmark function performs prediction.\n",
    "                 data_root_func: Callable[[], Path] = get_default_data_root # Function that returns the root directory where datasets are stored.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func # Now mandatory\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_func = test_func\n",
    "        self.init_window = init_window\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.pred_step = pred_step\n",
    "        self.data_root_func = data_root_func\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        \"\"\"Returns the evaluated data root path.\"\"\"\n",
    "        return self.data_root_func() \n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the dataset directory.\"\"\"\n",
    "        return self.data_root / self.dataset_id\n",
    "\n",
    "    # Reinstated ensure_dataset_exists method\n",
    "    def ensure_dataset_exists(self, force_download: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Checks if the dataset exists locally, downloads it if not or if forced.\n",
    "\n",
    "        Args:\n",
    "            force_download: If True, download the dataset even if it exists locally.\n",
    "        \"\"\"\n",
    "        dataset_path = self.dataset_path \n",
    "        download_func = self.download_func \n",
    "        if download_func is None:\n",
    "            print(f\"Warning: No download function specified for benchmark '{self.name}'. Cannot ensure data exists at {dataset_path}\")\n",
    "            # Check existence even if we can't download\n",
    "            if not dataset_path.is_dir():\n",
    "                 print(f\"Warning: Dataset directory {dataset_path} not found.\")\n",
    "            return\n",
    "\n",
    "        dataset_exists = dataset_path.is_dir()\n",
    "\n",
    "        if not dataset_exists or force_download:\n",
    "            print(f\"Dataset for '{self.name}' {'not found' if not dataset_exists else 'download forced'}. Preparing dataset at {dataset_path}...\")\n",
    "            self.data_root.mkdir(parents=True, exist_ok=True) \n",
    "            try:\n",
    "                download_func(dataset_path, force_download) \n",
    "                print(f\"Dataset '{self.name}' prepared successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing dataset '{self.name}': {e}\")\n",
    "                raise\n",
    "        else:\n",
    "             # Optionally print message if dataset already exists and not forced\n",
    "             # print(f\"Dataset for '{self.name}' found at {dataset_path}.\")\n",
    "             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal dummy loader - needed for tests below\n",
    "def _dummy_dataset_loader(\n",
    "    save_path: Path, # Directory where the dummy dataset files will be written\n",
    "    force_download: bool = False, # Argument for interface compatibility\n",
    "    create_train_valid_dir: bool = False # If True, create a 'train_valid' subdir as well\n",
    "    ):\n",
    "    \"\"\"Creates a dummy dataset structure with minimal HDF5 files for testing.\"\"\"\n",
    "    save_path = Path(save_path)\n",
    "    if save_path.is_dir() and not force_download: return\n",
    "\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    seq_len = 50\n",
    "    subdirs = ['train', 'valid', 'test']\n",
    "    if create_train_valid_dir:\n",
    "        subdirs.append('train_valid')\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = save_path / subdir\n",
    "        subdir_path.mkdir(exist_ok=True)\n",
    "        n_files = 1 if subdir == 'train_valid' else 2 # Create fewer files in train_valid for testing differentiation\n",
    "        for i in range(n_files):\n",
    "            dummy_file_path = subdir_path / f'{subdir}_{i}.hdf5'\n",
    "            try:\n",
    "                with h5py.File(dummy_file_path, 'w') as f:\n",
    "                    f.create_dataset('u0', data=np.random.rand(seq_len).astype(np.float32))\n",
    "                    f.create_dataset('u1', data=np.random.rand(seq_len).astype(np.float32))\n",
    "                    f.create_dataset('y0', data=np.random.rand(seq_len).astype(np.float32))\n",
    "                    f.attrs['fs'] = 10.0\n",
    "            except Exception as e: print(f\"Failed to create dummy file {dummy_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup shared for BenchmarkSpec Tests\n",
    "_test_data_dir_spec = Path('./_temp_identibench_data_spec_test')\n",
    "shutil.rmtree(_test_data_dir_spec, ignore_errors=True) # Clean before tests\n",
    "def _get_test_data_root_spec(): return _test_data_dir_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec basic initialization and defaults\n",
    "_spec_default = BenchmarkSpec(\n",
    "    name='_spec_default', dataset_id='_dummy_default',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_default.init_window, None)\n",
    "test_eq(_spec_default.pred_horizon, None)\n",
    "test_eq(_spec_default.pred_step, 1)\n",
    "test_eq(_spec_default.name, '_spec_default') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec initialization with prediction-related parameters\n",
    "_spec_pred_params = BenchmarkSpec(\n",
    "    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    init_window=20, pred_horizon=5, pred_step=2,\n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "test_eq(_spec_pred_params.init_window, 20)\n",
    "test_eq(_spec_pred_params.pred_horizon, 5)\n",
    "test_eq(_spec_pred_params.pred_step, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_spec_ensure' not found. Preparing dataset at _temp_identibench_data_spec_test/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n",
    "_spec_ensure = BenchmarkSpec(\n",
    "    name='_spec_ensure', dataset_id='_dummy_ensure',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    data_root_func=_get_test_data_root_spec\n",
    ")\n",
    "_spec_ensure.ensure_dataset_exists()\n",
    "_dataset_path_ensure = _spec_ensure.dataset_path\n",
    "test_eq(_dataset_path_ensure.is_dir(), True)\n",
    "test_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n",
    "_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists() \n",
    "_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_eq(_mtime_before_skip, _mtime_after_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_spec_ensure' download forced. Preparing dataset at _temp_identibench_data_spec_test/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n",
    "_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists(force_download=True) \n",
    "_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_ne(_mtime_before_force, _mtime_after_force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "shutil.rmtree(_test_data_dir_spec, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Internal helper function for loading raw sequences (no windowing)\n",
    "def _load_raw_sequences_from_files(\n",
    "    file_paths: List[Path], # List of HDF5 file paths to load from.\n",
    "    u_cols: List[str], # Input column names.\n",
    "    y_cols: List[str], # Output column names.\n",
    "    x_cols: Optional[List[str]], # Optional state column names.\n",
    ") -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "    \"\"\"\n",
    "    Loads and yields full sequences (u, y, x) from HDF5 files.\n",
    "    \"\"\"\n",
    "    if not file_paths: return iter([]) \n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                try:\n",
    "                    u_data = np.stack([f[col][()] for col in u_cols], axis=-1).astype(np.float32)\n",
    "                    y_data = np.stack([f[col][()] for col in y_cols], axis=-1).astype(np.float32)\n",
    "                    x_data = np.stack([f[col][()] for col in x_cols], axis=-1).astype(np.float32) if x_cols else None\n",
    "                except KeyError as e:\n",
    "                    print(f\"Warning: Column {e} not found in file {file_path}. Skipping file.\")\n",
    "                    continue\n",
    "\n",
    "                seq_len = u_data.shape[0]\n",
    "                if y_data.shape[0] != seq_len or (x_data is not None and x_data.shape[0] != seq_len):\n",
    "                     print(f\"Warning: Column length mismatch in {file_path}. Skipping file.\")\n",
    "                     continue\n",
    "\n",
    "                yield u_data, y_data, x_data \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingContext:\n",
    "    \"\"\"\n",
    "    Context object passed to the user's training function (`build_predictor`).\n",
    "\n",
    "    Holds the benchmark specification, hyperparameters, and seed.\n",
    "    Provides methods to access the raw, full-length training and validation data sequences.\n",
    "    Windowing/batching for training must be handled within the user's `build_predictor` function.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 spec: BenchmarkSpec, # The benchmark specification.\n",
    "                 hyperparameters: Dict[str, Any], # User-provided dictionary containing model and training hyperparameters.\n",
    "                 seed: Optional[int] = None # Optional random seed for reproducibility.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.spec = spec\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.seed = seed\n",
    "\n",
    "    # --- Data Access Methods ---\n",
    "\n",
    "    def _get_file_paths(self, subset: str) -> List[Path]:\n",
    "        \"\"\"Gets sorted list of HDF5 files for a given subset directory.\"\"\"\n",
    "        subset_path = self.spec.dataset_path / subset\n",
    "        if not subset_path.is_dir():\n",
    "            return []\n",
    "        return sorted(list(subset_path.glob('*.hdf5')))\n",
    "\n",
    "    def _get_sequences_from_subset(self, subset: str\n",
    "                                  ) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Loads raw sequences for a specific subset directory.\"\"\"\n",
    "        file_paths = self._get_file_paths(subset)\n",
    "        if not file_paths:\n",
    "             print(f\"Warning: No HDF5 files found in {self.spec.dataset_path / subset}. Returning empty iterator.\")\n",
    "             return iter([])\n",
    "\n",
    "        return _load_raw_sequences_from_files(\n",
    "            file_paths=file_paths,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_train_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'train' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('train')\n",
    "\n",
    "    def get_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'valid' subset.\"\"\"\n",
    "        return self._get_sequences_from_subset('valid')\n",
    "\n",
    "    def get_train_valid_sequences(self) -> Iterator[Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding raw (u, y, x) tuples for combined training and validation.\n",
    "\n",
    "        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.\n",
    "        If not, it loads data from 'train' and 'valid' subsets sequentially.\n",
    "        \"\"\"\n",
    "        train_valid_files = self._get_file_paths('train_valid')\n",
    "        if train_valid_files:\n",
    "            return _load_raw_sequences_from_files(\n",
    "                file_paths=train_valid_files, u_cols=self.spec.u_cols, y_cols=self.spec.y_cols,\n",
    "                x_cols=self.spec.x_cols\n",
    "            )\n",
    "        else:\n",
    "            train_iter = self._get_sequences_from_subset('train')\n",
    "            valid_iter = self._get_sequences_from_subset('valid')\n",
    "            return itertools.chain(train_iter, valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup shared for TrainingContext Tests\n",
    "_test_data_dir_ctx = Path('./_temp_identibench_data_ctx_test')\n",
    "_test_data_dir_ctx_tv = Path('./_temp_identibench_data_ctx_tv_test') \n",
    "shutil.rmtree(_test_data_dir_ctx, ignore_errors=True)\n",
    "shutil.rmtree(_test_data_dir_ctx_tv, ignore_errors=True)\n",
    "def _get_test_data_root_ctx(): return _test_data_dir_ctx\n",
    "def _get_test_data_root_ctx_tv(): return _test_data_dir_ctx_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_dummy_ctx_base' not found. Preparing dataset at _temp_identibench_data_ctx_test/_dummy_dataset_ctx_base...\n",
      "Dataset '_dummy_ctx_base' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create base dummy data (no train_valid dir) \n",
    "_dummy_spec_ctx_base = BenchmarkSpec(\n",
    "    name='_dummy_ctx_base', dataset_id='_dummy_dataset_ctx_base',\n",
    "    u_cols=['u0', 'u1'], y_cols=['y0'],metric_func=identibench.metrics.rmse, \n",
    "    download_func=lambda p, f: _dummy_dataset_loader(p, force_download=f, create_train_valid_dir=False),\n",
    "    data_root_func=_get_test_data_root_ctx, init_window=10\n",
    ")\n",
    "_dummy_spec_ctx_base.ensure_dataset_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for '_dummy_ctx_tv' not found. Preparing dataset at _temp_identibench_data_ctx_tv_test/_dummy_dataset_ctx_tv...\n",
      "Dataset '_dummy_ctx_tv' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create dummy data WITH train_valid dir \n",
    "_dummy_spec_ctx_tv = BenchmarkSpec(\n",
    "    name='_dummy_ctx_tv', dataset_id='_dummy_dataset_ctx_tv',\n",
    "    u_cols=['u0', 'u1'], y_cols=['y0'],metric_func=identibench.metrics.rmse, \n",
    "    download_func=lambda p, f: _dummy_dataset_loader(p, force_download=f, create_train_valid_dir=True),\n",
    "    data_root_func=_get_test_data_root_ctx_tv, init_window=10\n",
    ")\n",
    "_dummy_spec_ctx_tv.ensure_dataset_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Shared constants for tests\n",
    "_seq_len_ctx = 50 \n",
    "_n_files_train_valid_ctx = 2 \n",
    "_n_files_tv_dir_ctx = 1 \n",
    "_hyperparams_ctx = {'lr': 0.01, 'hidden': 64}\n",
    "_seed_ctx = 42\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 16\n",
    "# Test: TrainingContext initialization\n",
    "_ctx = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "test_eq(_ctx.spec, _dummy_spec_ctx_base)\n",
    "test_eq(_ctx.hyperparameters, _hyperparams_ctx)\n",
    "test_eq(_ctx.seed, _seed_ctx)\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 17\n",
    "# Test: TrainingContext get_train_sequences\n",
    "_ctx = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_train_sequences = list(_ctx.get_train_sequences())\n",
    "test_eq(len(_train_sequences), _n_files_train_valid_ctx) \n",
    "_u_train, _y_train, _x_train = _train_sequences[0]\n",
    "test_eq(_u_train.shape, (_seq_len_ctx, len(_dummy_spec_ctx_base.u_cols))) \n",
    "test_eq(_y_train.shape, (_seq_len_ctx, len(_dummy_spec_ctx_base.y_cols)))\n",
    "test_eq(_x_train, None)\n",
    "test_eq(_u_train.dtype, np.float32)\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 18\n",
    "# Test: TrainingContext get_valid_sequences\n",
    "_ctx = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_valid_sequences = list(_ctx.get_valid_sequences())\n",
    "test_eq(len(_valid_sequences), _n_files_train_valid_ctx)\n",
    "_u_valid, _y_valid, _x_valid = _valid_sequences[0]\n",
    "test_eq(_u_valid.shape, (_seq_len_ctx, len(_dummy_spec_ctx_base.u_cols)))\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 19\n",
    "# Test: TrainingContext get_train_valid_sequences - fallback (no train_valid dir)\n",
    "_ctx_tv_fallback = TrainingContext(spec=_dummy_spec_ctx_base, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_tv_sequences_fallback = list(_ctx_tv_fallback.get_train_valid_sequences())\n",
    "test_eq(len(_tv_sequences_fallback), _n_files_train_valid_ctx + _n_files_train_valid_ctx)\n",
    "_u_tv_fb_train, _y_tv_fb_train, _ = _tv_sequences_fallback[0] \n",
    "test_eq(_u_tv_fb_train.shape[0], _seq_len_ctx)\n",
    "_u_tv_fb_valid, _y_tv_fb_valid, _ = _tv_sequences_fallback[_n_files_train_valid_ctx] \n",
    "test_eq(_u_tv_fb_valid.shape[0], _seq_len_ctx)\n",
    "\n",
    "# %% ../nbs/benchmark.ipynb 20\n",
    "# Test: TrainingContext get_train_valid_sequences - direct (train_valid dir exists)\n",
    "_ctx_tv_direct = TrainingContext(spec=_dummy_spec_ctx_tv, hyperparameters=_hyperparams_ctx, seed=_seed_ctx)\n",
    "_tv_sequences_direct = list(_ctx_tv_direct.get_train_valid_sequences())\n",
    "test_eq(len(_tv_sequences_direct), _n_files_tv_dir_ctx)\n",
    "_u_tv_direct, _y_tv_direct, _ = _tv_sequences_direct[0]\n",
    "test_eq(_u_tv_direct.shape[0], _seq_len_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "shutil.rmtree(_test_data_dir_ctx, ignore_errors=True)\n",
    "shutil.rmtree(_test_data_dir_ctx_tv, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
