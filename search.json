[
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Data Utilities",
    "section": "",
    "text": "First, we evaluate how the datasets are loaded by the nonlinear_benchmarks library\nsource",
    "crumbs": [
      "Data Utilities"
    ]
  },
  {
    "objectID": "utils.html#test-utilities",
    "href": "utils.html#test-utilities",
    "title": "Data Utilities",
    "section": "Test utilities",
    "text": "Test utilities\n\nsource\n\nhdf_files_from_path\n\n hdf_files_from_path (fpath:pathlib.Path)",
    "crumbs": [
      "Data Utilities"
    ]
  },
  {
    "objectID": "utils.html#data-utilities",
    "href": "utils.html#data-utilities",
    "title": "Data Utilities",
    "section": "data utilities",
    "text": "data utilities\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark()\nplt.plot(train_val.y)\ntype(train_val)\n\nnonlinear_benchmarks.utilities.Input_output_data\n\n\n\n\n\n\n\n\n\nThe data is store in a Input_output_data class, which provides customized access. We want to write a function, which exports the underlying data to hdf5 files.\n\nu = train_val.atleast_2d().u\nu.shape\n\n(100000, 1)\n\n\n\nsource\n\nwrite_dataset\n\n write_dataset (group:h5py._hl.files.File|h5py._hl.group.Group,\n                ds_name:str, data:numpy.ndarray, dtype:str='f4',\n                chunks:tuple[int,...]|None=None)\n\n\ntmp_dir = Path('./tmp/')\nos.makedirs(tmp_dir,exist_ok=True)\ntmp_file = tmp_dir / 'tmp.hdf5'\nwith h5py.File(tmp_file,'w') as f:\n    write_dataset(f,'u',u)\n\nwith h5py.File(tmp_file,'r') as f:\n    hdf_u = f['u'][:]\n    test_ne(hdf_u.dtype,u.dtype)\n    test_eq(hdf_u,u.astype('f4'))\n\n\nsource\n\n\nwrite_array\n\n write_array (group:h5py._hl.files.File|h5py._hl.group.Group, ds_name:str,\n              data:numpy.ndarray, dtype:str='f4',\n              chunks:tuple[int,...]|None=None)\n\nWrites a 2d numpy array rowwise to a hdf5 file.\n\nwith h5py.File(tmp_file,'w') as f:\n    write_array(f,'u',u)\n\nwith h5py.File(tmp_file,'r') as f:\n    hdf_u = f['u0'][:]\n    test_ne(hdf_u.dtype,u.dtype)\n    test_ne(hdf_u,u.astype('f4'))\n    test_eq(hdf_u[:,None],u.astype('f4'))\n\n\nsource\n\n\niodata_to_hdf5\n\n iodata_to_hdf5 (iodata:nonlinear_benchmarks.utilities.Input_output_data,\n                 hdf_dir:pathlib.Path, f_name:str=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\niodata\nInput_output_data\n\ndata to save to file\n\n\nhdf_dir\nPath\n\nExport directory for hdf5 files\n\n\nf_name\nstr\nNone\nname of hdf5 file without ‘.hdf5’ ending\n\n\nReturns\nPath\n\n\n\n\n\n\nfname = iodata_to_hdf5(train_val,tmp_dir)\n\nwith h5py.File(fname,'r') as f:\n    hdf_u = f['u0'][:]\n    hdf_y = f['y0'][:]\n    test_eq(hdf_u[:,None],train_val.atleast_2d().u.astype('f4'))\n    test_eq(hdf_y[:,None],train_val.atleast_2d().y.astype('f4'))\n\nLet us evaluate how the general shape of the downloaded datasets looks like\n\nfor bench in nonlinear_benchmarks.all_splitted_benchmarks:\n    train,test = bench(atleast_2d=True,always_return_tuples_of_datasets=True)\n    print(type(train))\n    print(type(train[0]))\n    break\n\n&lt;class 'tuple'&gt;\n&lt;class 'nonlinear_benchmarks.utilities.Input_output_data'&gt;\n\n\nWith the correct flags set, all datasets have a consistent training and test tuple of one or more elements of type Input_output_data. We will transform that in a training, validation and test tuple, which we will then save with a single function.\n\n# for bench in nonlinear_benchmarks.all_not_splitted_benchmarks:\n#     train = bench()\n#     if len(train) == 2:\n#         train, test = train\n#         # print('\\n'.join(map(str,train)))\n#         if isinstance(train,list):\n#             print(len(train))\n#             print(train[0].name)\n\nOnly the datasets in nonlinear_benchmarks.all_splitted_benchmarks have a consistent output form. The other benchmarks have random splits\n\nsource\n\n\ndataset_to_hdf5\n\n dataset_to_hdf5 (train:tuple, valid:tuple, test:tuple,\n                  save_path:pathlib.Path, train_valid:tuple=None)\n\nSave a dataset consisting of training, validation, and test set in hdf5 format in seperate subdirectories\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain\ntuple\n\ntuple of Input_output_data for training\n\n\nvalid\ntuple\n\ntuple of Input_output_data for validation\n\n\ntest\ntuple\n\ntuple of Input_output_data for test\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\ntrain_valid\ntuple\nNone\noptional tuple of unsplit Input_output_data for training and validation\n\n\nReturns\nNone\n\n\n\n\n\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark()\nsplit_idx = 90_000\ntrain = train_val[:split_idx]\nvalid = train_val[split_idx:]\ntest = test\n\n\ndataset_to_hdf5(train,valid,test,tmp_dir)\ndataset_to_hdf5((train,),valid,test,tmp_dir)",
    "crumbs": [
      "Data Utilities"
    ]
  },
  {
    "objectID": "utils.html#download-utilities",
    "href": "utils.html#download-utilities",
    "title": "Data Utilities",
    "section": "Download Utilities",
    "text": "Download Utilities\n\nsource\n\nunzip_download\n\n unzip_download (url:str, extract_dir:pathlib.Path=Path('.'))\n\ndownloads a zip archive to ram and extracts it\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nurl to file to download\n\n\nextract_dir\nPath\n.\ndirectory the archive is extracted to\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nunrar_download\n\n unrar_download (url:str, extract_dir:pathlib.Path=Path('.'))\n\ndownloads a rar archive to ram and extracts it\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nurl to file to download\n\n\nextract_dir\nPath\n.\ndirectory the archive is extracted to\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\ndownload\n\n download (url:str, target_dir:pathlib.Path=Path('.'))\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nurl to file to download\n\n\ntarget_dir\nPath\n.\n\n\n\nReturns\nPath",
    "crumbs": [
      "Data Utilities"
    ]
  },
  {
    "objectID": "datasets/workshop.html",
    "href": "datasets/workshop.html",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "",
    "text": "tmp_dir = idb.get_default_data_root()",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#wiener-hammerstein-dataset",
    "href": "datasets/workshop.html#wiener-hammerstein-dataset",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "Wiener Hammerstein Dataset",
    "text": "Wiener Hammerstein Dataset\n\ndef plot_workshop_data(dataset_function,max_sequences=3):\n    train_val, test = dataset_function(always_return_tuples_of_datasets=True)\n    fig, axs = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n    # Plot training/validation data\n    for i, data in enumerate(train_val[:max_sequences]):\n        axs[0].plot(data.u, alpha=0.7, label=f'Train {i+1}')\n        axs[1].plot(data.y, alpha=0.7, label=f'Train {i+1}')\n\n    # Plot test data\n    for i, data in enumerate(test[:max_sequences]):\n        axs[0].plot(data.u, ls='--', alpha=0.8, label=f'Test {i+1}')\n        axs[1].plot(data.y, ls='--', alpha=0.8, label=f'Test {i+1}')\n\n    axs[0].set_title('Input Sequences')\n    axs[0].set_ylabel('Amplitude')\n    axs[0].legend()\n\n    axs[1].set_title('Output Sequences')\n    axs[1].set_xlabel('Sample')\n    axs[1].set_ylabel('Amplitude')\n    axs[1].legend()\n\n    plt.tight_layout()\nplot_workshop_data(nonlinear_benchmarks.WienerHammerBenchMark)\n\n\n\n\n\n\n\n\n\nsource\n\ndl_wiener_hammerstein\n\n dl_wiener_hammerstein (save_path:pathlib.Path, force_download:bool=False,\n                        save_train_valid:bool=True, split_idx:int=80000)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nsave_train_valid\nbool\nTrue\nsave unsplitted train and valid datasets in ‘train_valid’ subdirectory\n\n\nsplit_idx\nint\n80000\nsplit index for train and valid datasets\n\n\nReturns\nNone\n\n\n\n\n\n\ndl_wiener_hammerstein(tmp_dir / 'wh' )\ndl_wiener_hammerstein(tmp_dir / 'wh' ,save_train_valid=False)\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkWH_Simulation, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkWH_Simulation, seed: 274805046\n\n\n247.20421449579186\n\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkWH_Prediction, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkWH_Prediction, seed: 1203809552\n\n\n241.88528087531387",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#silverbox-dataset",
    "href": "datasets/workshop.html#silverbox-dataset",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "Silverbox Dataset",
    "text": "Silverbox Dataset\n\nplot_workshop_data(nonlinear_benchmarks.Silverbox)\n\n\n\n\n\n\n\n\n\nsource\n\ndl_silverbox\n\n dl_silverbox (save_path:pathlib.Path, force_download:bool=False,\n               save_train_valid:bool=True, split_idx:int=50000)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nsave_train_valid\nbool\nTrue\nsave unsplitted train and valid datasets in ‘train_valid’ subdirectory\n\n\nsplit_idx\nint\n50000\nsplit index for train and valid datasets\n\n\nReturns\nNone\n\n\n\n\n\n\ndl_silverbox(tmp_dir / 'silverbox')\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkSilverbox_Simulation, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkSilverbox_Simulation, seed: 665849708\n\n\n50.27112906124001\n\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkSilverbox_Prediction, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkSilverbox_Prediction, seed: 1014722045\n\n\n45.73670007667041",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#cascaded-tanks-dataset",
    "href": "datasets/workshop.html#cascaded-tanks-dataset",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "Cascaded Tanks Dataset",
    "text": "Cascaded Tanks Dataset\n\nplot_workshop_data(nonlinear_benchmarks.Cascaded_Tanks)\n\n\n\n\n\n\n\n\n\nsource\n\ndl_cascaded_tanks\n\n dl_cascaded_tanks (save_path:pathlib.Path, force_download:bool=False,\n                    save_train_valid:bool=True, split_idx:int=160)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nsave_train_valid\nbool\nTrue\nsave unsplitted train and valid datasets in ‘train_valid’ subdirectory\n\n\nsplit_idx\nint\n160\nsplit index for train and valid datasets\n\n\nReturns\nNone\n\n\n\n\n\n\ndl_cascaded_tanks(tmp_dir  / 'cascaded_tanks' )\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkCascadedTanks_Simulation, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkCascadedTanks_Simulation, seed: 2125691036\n\n\n6.190198457201898\n\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkCascadedTanks_Prediction, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkCascadedTanks_Prediction, seed: 3952414853\n\n\n6.087247931804716",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#emps-dataset",
    "href": "datasets/workshop.html#emps-dataset",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "EMPS Dataset",
    "text": "EMPS Dataset\n\nplot_workshop_data(nonlinear_benchmarks.EMPS)\n\n\n\n\n\n\n\n\n\nsource\n\ndl_emps\n\n dl_emps (save_path:pathlib.Path, force_download:bool=False,\n          save_train_valid:bool=True, split_idx:int=18000)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nsave_train_valid\nbool\nTrue\nsave unsplitted train and valid datasets in ‘train_valid’ subdirectory\n\n\nsplit_idx\nint\n18000\nsplit index for train and valid datasets\n\n\nReturns\nNone\n\n\n\n\n\n\ndl_emps(tmp_dir  / 'emps')\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkEMPS_Simulation, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkEMPS_Simulation, seed: 159203475\n\n\n148.94509370511423\n\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkEMPS_Prediction, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkEMPS_Prediction, seed: 1758391957\n\n\n127.00285207332765",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#noisy-wiener-hammerstein",
    "href": "datasets/workshop.html#noisy-wiener-hammerstein",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "Noisy Wiener Hammerstein",
    "text": "Noisy Wiener Hammerstein\n\nsource\n\ndl_noisy_wh\n\n dl_noisy_wh (save_path:pathlib.Path, force_download:bool=False,\n              save_train_valid:bool=True)\n\nthe wiener hammerstein dataset with process noise\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nsave_train_valid\nbool\nTrue\nsave unsplitted train and valid datasets in ‘train_valid’ subdirectory\n\n\nReturns\nNone\n\n\n\n\n\n\ndl_noisy_wh(tmp_dir / 'noisy_wh' )\n\n\nresults = result = idb.run_benchmark(\n    spec=BenchmarkNoisyWH_Simulation, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkNoisyWH_Simulation, seed: 3771735968\n\n\n104.1542183129001\n\n\n\nresults = result = idb.run_benchmark(\n    spec=BenchmarkNoisyWH_Prediction, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkNoisyWH_Prediction, seed: 421895102\n\n\n81.6161143620699",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#parallel-wienerhammerstein",
    "href": "datasets/workshop.html#parallel-wienerhammerstein",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "Parallel Wienerhammerstein",
    "text": "Parallel Wienerhammerstein\n\n#ToDo",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#f16",
    "href": "datasets/workshop.html#f16",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "F16",
    "text": "F16\n\n#ToDo",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/workshop.html#coupled-electric-drives",
    "href": "datasets/workshop.html#coupled-electric-drives",
    "title": "Nonlinear Benchmark Workshop Datasets",
    "section": "Coupled Electric Drives",
    "text": "Coupled Electric Drives\n\nplot_workshop_data(nonlinear_benchmarks.CED)\n\n\n\n\n\n\n\n\n\nsource\n\ndl_ced\n\n dl_ced (save_path:pathlib.Path, force_download:bool=False,\n         save_train_valid:bool=True, split_idx:int=300)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nsave_train_valid\nbool\nTrue\nsave unsplitted train and valid datasets in ‘train_valid’ subdirectory\n\n\nsplit_idx\nint\n300\nsplit index for train and valid datasets\n\n\nReturns\nNone\n\n\n\n\n\n\ndl_ced(tmp_dir / 'ced' )\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkCED_Simulation, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkCED_Simulation, seed: 750852011\n\n\n0.5816430221160263\n\n\n\nresult = idb.run_benchmark(\n    spec=BenchmarkCED_Prediction, \n    build_model=idb._dummy_build_model\n)\nresult['metric_score']\n\nBuilding model with spec: BenchmarkCED_Prediction, seed: 4123995934\n\n\n0.49837518028637195",
    "crumbs": [
      "datasets",
      "Nonlinear Benchmark Workshop Datasets"
    ]
  },
  {
    "objectID": "datasets/ship.html",
    "href": "datasets/ship.html",
    "title": "Ship Dataset",
    "section": "",
    "text": "source\n\ndl_ship\n\n dl_ship (save_path:pathlib.Path, force_download:bool=False,\n          remove_download:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nremove_download\nbool\nTrue\n\n\n\nReturns\nNone\n\n\n\n\n\n\ntmp_dir = idb.get_default_data_root()\ndl_ship(tmp_dir / 'ship')\n\n/Users/daniel/Development/identibench/.venv/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \n\"ipywidgets\" for Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n\n\n\n\n🎉 Connected to 'https://darus.uni-stuttgart.de/'\n\n\n\n\n\n\nFetching dataset 'doi:10.18419/darus-2905' from 'https://darus.uni-stuttgart.de/'\n\n\n\n\n╭────────────────────────────────────────── Dataset Information ──────────────────────────────────────────╮\n│ Title: A Simulated 4-DOF Ship Motion Dataset for System Identification under Environmental Disturbances │\n│ Version: latest                                                                                         │\n│ Files: 126                                                                                              │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nDownloading files\n\n\n\n\n\n\n\n╰── ✅ Done \n\n\n\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkShip_Simulation, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkShip_Simulation, seed: 3748348152\n\n\n1.1163076096434674\n\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkShip_Prediction, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkShip_Prediction, seed: 4165156300\n\n\n1.0794470890247279",
    "crumbs": [
      "datasets",
      "Ship Dataset"
    ]
  },
  {
    "objectID": "datasets/quadrotor_pi.html",
    "href": "datasets/quadrotor_pi.html",
    "title": "Quadrotor PI Dataset",
    "section": "",
    "text": "source\n\ndl_quad_pi\n\n dl_quad_pi (save_path:pathlib.Path, force_download:bool=False,\n             remove_download:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nremove_download\nbool\nFalse\nremove downloaded zip/extracted bags afterwards\n\n\nReturns\nNone\n\n\n\n\n\n\ntmp_dir = idb.get_default_data_root()\ndl_quad_pi(tmp_dir / 'quad_pi')\n# quad_pi(tmp_dir / 'quad_pi',force_download=True)\n\nFile exists: /Users/daniel/Library/Application Support/nonlinear_benchmarks/Quadrotor_pi/bags.zip\n\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkQuadPi_Simulation, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkQuadPi_Simulation, seed: 3951643791\n\n\n3.9486249384857373\n\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkQuadPi_Prediction, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkQuadPi_Prediction, seed: 1289423199\n\n\n3.698025508463279",
    "crumbs": [
      "datasets",
      "Quadrotor PI Dataset"
    ]
  },
  {
    "objectID": "index.html#identibench",
    "href": "index.html#identibench",
    "title": "identibench",
    "section": "IdentiBench",
    "text": "IdentiBench\n   \nIdentiBench is a Python library designed to streamline and standardize the benchmarking of system identification models. Evaluating and comparing dynamic models often requires repetitive setup for data handling, evaluation protocols, and metrics implementation, making fair comparisons and reproducing results challenging. IdentiBench tackles this by offering a collection of pre-defined benchmark specifications for simulation and prediction tasks, built upon common datasets. It automates data downloading and processing into a consistent format and provides standard evaluation metrics via a simple interface (run_benchmark). This allows you to focus your efforts on developing innovative models, while relying on IdentiBench for robust and reproducible evaluation.",
    "crumbs": [
      "IdentiBench"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "identibench",
    "section": "Key Features",
    "text": "Key Features\n\nAccess Many Benchmarks from different systems: Instantly utilize pre-configured benchmarks covering diverse domains like electronics (Silverbox), mechanics (Industrial Robot), process control (Cascaded Tanks), aerospace (Quadrotors), and more, available for both simulation and prediction tasks.\nAutomate Data Management: Forget manual downloading and processing; the library handles fetching data from various sources (web, Drive, Dataverse), extracting archives (ZIP, RAR, MAT, BAG), converting to a standard HDF5 format, and caching locally.\nIntegrate Any Model to evaluate on all benchmarks: Plug in your custom models, regardless of the Python framework used (NumPy, SciPy, PyTorch, TensorFlow, JAX, etc.), using a straightforward function interface (build_model) that receives all necessary context.\nCapture Comprehensive Results: Obtain detailed evaluation reports including standard metrics (RMSE, NRMSE, FIT%, etc.), task-specific scores, execution timings, configuration parameters (hyperparameters, seed), and raw model predictions for thorough analysis.\nEasily Define New Benchmarks: Go beyond the included datasets by creating your own benchmark specifications (BenchmarkSpecSimulation, BenchmarkSpecPrediction) for private data or unique tasks, leveraging the library’s structure and transparent data format.",
    "crumbs": [
      "IdentiBench"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "identibench",
    "section": "Installation",
    "text": "Installation\nYou can install identibench using pip:\npip install identibench\nTo install the latest development version directly from GitHub, use:\npip install git+https://github.com/daniel-om-weber/identibench.git\n\n# Basic usage\nimport identibench as idb\nfrom pathlib import Path\n\n# Example: Download a single dataset\n# Note: Always use a Path object, not a string\nsave_path = Path('./tmp/wh')\nidb.datasets.workshop.dl_wiener_hammerstein(save_path)\n\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.parameter_estimation import LeastSquares\ndef build_frols_model(context):\n    u_train, y_train, _ = next(context.get_train_sequences())\n    \n    ylag = context.hyperparameters.get('ylag', 5)\n    xlag = context.hyperparameters.get('xlag', 5)\n    n_terms = context.hyperparameters.get('n_terms', 10)\n    estimator = context.hyperparameters.get('estimator', LeastSquares())\n\n    _model = FROLS(xlag=xlag, ylag=ylag, n_terms=n_terms,estimator=estimator)\n    _model.fit(X=u_train, y=y_train)\n\n    def model(u_test, y_init):\n        nonlocal _model\n        yhat_full = _model.predict(X=u_test, y=y_init[:_model.max_lag])\n        y_pred = yhat_full[_model.max_lag:]\n        return y_pred\n    \n    return model\n\n\nhyperparams = {\n    'ylag': 2,\n    'xlag': 2,\n    'n_terms': 10, # Number of terms for FROLS\n    'estimator': LeastSquares()\n}\n\nresults = idb.run_benchmark(\n    spec=idb.BenchmarkWH_Simulation,\n    build_model=build_frols_model,\n    hyperparameters=hyperparams\n)",
    "crumbs": [
      "IdentiBench"
    ]
  },
  {
    "objectID": "index.html#simulation-benchmarks",
    "href": "index.html#simulation-benchmarks",
    "title": "identibench",
    "section": "Simulation Benchmarks",
    "text": "Simulation Benchmarks\n\n\n\nKey\nBenchmark Name\n\n\n\n\nWH_Sim\nBenchmarkWH_Simulation\n\n\nSilverbox_Sim\nBenchmarkSilverbox_Simulation\n\n\nTanks_Sim\nBenchmarkCascadedTanks_Simulation\n\n\nCED_Sim\nBenchmarkCED_Simulation\n\n\nEMPS_Sim\nBenchmarkEMPS_Simulation\n\n\nNoisyWH_Sim\nBenchmarkNoisyWH_Simulation\n\n\nRobotForward_Sim\nBenchmarkRobotForward_Simulation\n\n\nRobotInverse_Sim\nBenchmarkRobotInverse_Simulation\n\n\nShip_Sim\nBenchmarkShip_Simulation\n\n\nQuadPelican_Sim\nBenchmarkQuadPelican_Simulation\n\n\nQuadPi_Sim\nBenchmarkQuadPi_Simulation",
    "crumbs": [
      "IdentiBench"
    ]
  },
  {
    "objectID": "index.html#prediction-benchmarks",
    "href": "index.html#prediction-benchmarks",
    "title": "identibench",
    "section": "Prediction Benchmarks",
    "text": "Prediction Benchmarks\n\n\n\nKey\nBenchmark Name\n\n\n\n\nWH_Pred\nBenchmarkWH_Prediction\n\n\nSilverbox_Pred\nBenchmarkSilverbox_Prediction\n\n\nTanks_Pred\nBenchmarkCascadedTanks_Prediction\n\n\nCED_Pred\nBenchmarkCED_Prediction\n\n\nEMPS_Pred\nBenchmarkEMPS_Prediction\n\n\nNoisyWH_Pred\nBenchmarkNoisyWH_Prediction\n\n\nRobotForward_Pred\nBenchmarkRobotForward_Prediction\n\n\nRobotInverse_Pred\nBenchmarkRobotInverse_Prediction\n\n\nShip_Pred\nBenchmarkShip_Prediction\n\n\nQuadPelican_Pred\nBenchmarkQuadPelican_Prediction\n\n\nQuadPi_Pred\nBenchmarkQuadPi_Prediction",
    "crumbs": [
      "IdentiBench"
    ]
  },
  {
    "objectID": "index.html#workflow-details",
    "href": "index.html#workflow-details",
    "title": "identibench",
    "section": "Workflow Details",
    "text": "Workflow Details\nThis section provides more detail on the core concepts and components of the identibench workflow.\n\nBenchmark Types\nidentibench defines two main types of benchmark tasks, specified using different classes:\n\nSimulation (BenchmarkSpecSimulation):\n\nGoal: Evaluate a model’s ability to perform a free-run simulation, predicting the system’s output over an extended period given the input sequence.\nTypical Input to Predictor: The full input sequence (u_test) and potentially an initial segment of the output sequence (y_test[:init_window]) for warm-up or state initialization.\nExpected Output from Predictor: The predicted output sequence (y_pred) corresponding to the input, usually excluding the warm-up period.\nUse Case: Assessing models intended for long-term prediction, control simulation, or understanding overall system dynamics.\n\nPrediction (BenchmarkSpecPrediction):\n\nGoal: Evaluate a model’s ability to predict the system’s output k steps into the future based on recent past data.\nTypical Input to Predictor: Often involves windows of past inputs and outputs (e.g., u[t:t+H], y[t:t+H]).\nExpected Output from Predictor: The predicted output at a specific future time step (e.g., y[t+H+k]). The pred_horizon parameter defines ‘k’, and pred_step defines how frequently predictions are made.\nUse Case: Evaluating models focused on short-to-medium term forecasting, state estimation, or receding horizon control.\n\ninit_window: Both benchmark types often use an init_window. This specifies an initial number of time steps whose data might be provided to the model for initialization or warm-up. Importantly, data within this window is typically excluded from the final performance metric calculation to ensure a fair evaluation of the model’s predictive capabilities beyond the initial transient.\n\n\n\nModel Interface (build_model)\nThe core of integrating your custom logic is the build_model function you provide to run_benchmark.\n\nPurpose: This function is responsible for defining your model architecture, training it using the provided data, and returning a callable predictor function.\nInput (context: TrainingContext): Your build_model function receives a single argument, context, which is a TrainingContext object. This object gives you access to:\n\ncontext.spec: The full specification of the current benchmark being run (including dataset paths, input/output columns, init_window, etc.).\ncontext.hyperparameters: A dictionary containing any hyperparameters you passed to run_benchmark. Use this to configure your model or training process.\ncontext.seed: A random seed for ensuring reproducibility.\nData Access Methods: Functions like context.get_train_sequences() and context.get_valid_sequences() provide iterators over the raw, full-length training and validation data sequences (as tuples of NumPy arrays (u, y, x)). Note: You need to handle any batching or windowing required for your specific training algorithm within your build_model function.\n\nOutput (Predictor Callable): build_model must return a callable object (e.g., a function, an object’s method) that represents your trained model ready for prediction/simulation. This returned callable will be used internally by run_benchmark on the test set. Its expected signature depends on the benchmark type, but typically it accepts NumPy arrays for test inputs (and potentially initial outputs) and returns a NumPy array containing the predictions.\n\n\n\nRunning Multiple Benchmarks\nTo evaluate a model across several scenarios efficiently, use the run_multiple_benchmarks function:\n\n# Example: Run on a subset of benchmarks\nspecs_to_run = {\n    'WH_Sim': idb.simulation_benchmarks['WH_Sim'],\n    'Silverbox_Sim': idb.simulation_benchmarks['Silverbox_Sim']\n}\n\n# Assume 'my_build_model' is your defined build function\nall_results = idb.run_benchmarks(specs_to_run, build_model=build_frols_model,n_times=3)\n\nall_results\n\n--- Starting benchmark run for 2 specifications, repeating each 3 times ---\n\n-- Repetition 1/3 --\n\n[1/6] Running: BenchmarkWH_Simulation (Rep 1)\n  -&gt; Success: BenchmarkWH_Simulation (Rep 1) completed.\n\n[2/6] Running: BenchmarkSilverbox_Simulation (Rep 1)\n  -&gt; Success: BenchmarkSilverbox_Simulation (Rep 1) completed.\n\n-- Repetition 2/3 --\n\n[3/6] Running: BenchmarkWH_Simulation (Rep 2)\n  -&gt; Success: BenchmarkWH_Simulation (Rep 2) completed.\n\n[4/6] Running: BenchmarkSilverbox_Simulation (Rep 2)\n  -&gt; Success: BenchmarkSilverbox_Simulation (Rep 2) completed.\n\n-- Repetition 3/3 --\n\n[5/6] Running: BenchmarkWH_Simulation (Rep 3)\n  -&gt; Success: BenchmarkWH_Simulation (Rep 3) completed.\n\n[6/6] Running: BenchmarkSilverbox_Simulation (Rep 3)\n  -&gt; Success: BenchmarkSilverbox_Simulation (Rep 3) completed.\n\n--- Benchmark run finished. 6/6 individual runs completed successfully. ---\n\n\n\n\n\n\n\n\n\nbenchmark_name\ndataset_id\nhyperparameters\nseed\ntraining_time_seconds\ntest_time_seconds\nbenchmark_type\nmetric_name\nmetric_score\ncs_multisine_rmse\ncs_arrow_full_rmse\ncs_arrow_no_extrapolation_rmse\n\n\n\n\n0\nBenchmarkWH_Simulation\nwh\n{}\n2406651230\n4.944649\n1.012850\nBenchmarkSpecSimulation\nrmse_mV\n42.161572\nNaN\nNaN\nNaN\n\n\n1\nBenchmarkSilverbox_Simulation\nsilverbox\n{}\n3813113752\n2.839149\n1.246224\nBenchmarkSpecSimulation\nrmse_mV\n10.732386\n8.501941\n16.154317\n7.5409\n\n\n2\nBenchmarkWH_Simulation\nwh\n{}\n1950649438\n4.801520\n1.034119\nBenchmarkSpecSimulation\nrmse_mV\n42.161572\nNaN\nNaN\nNaN\n\n\n3\nBenchmarkSilverbox_Simulation\nsilverbox\n{}\n1560698088\n2.880391\n1.217932\nBenchmarkSpecSimulation\nrmse_mV\n10.732386\n8.501941\n16.154317\n7.5409\n\n\n4\nBenchmarkWH_Simulation\nwh\n{}\n3258007268\n4.916941\n1.021927\nBenchmarkSpecSimulation\nrmse_mV\n42.161572\nNaN\nNaN\nNaN\n\n\n5\nBenchmarkSilverbox_Simulation\nsilverbox\n{}\n4194043971\n2.937101\n1.231710\nBenchmarkSpecSimulation\nrmse_mV\n10.732386\n8.501941\n16.154317\n7.5409\n\n\n\n\n\n\n\nThis function iterates through the provided list or dictionary of benchmark specifications, calling run_benchmark for each one using the same build_model function and hyperparameters.\n\n#calculate mean and std of the results\nidb.aggregate_benchmark_results(all_results,agg_funcs=['mean','std'])\n\n\n\n\n\n\n\n\ntraining_time_seconds\ntest_time_seconds\nmetric_score\ncs_multisine_rmse\ncs_arrow_full_rmse\ncs_arrow_no_extrapolation_rmse\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nbenchmark_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarkSilverbox_Simulation\n2.885547\n0.049179\n1.231955\n0.014147\n10.732386\n0.0\n8.501941\n0.0\n16.154317\n0.0\n7.5409\n0.0\n\n\nBenchmarkWH_Simulation\n4.887703\n0.075912\n1.022966\n0.010673\n42.161572\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nData Handling & Format\nUnderstanding how identibench organizes and stores data is helpful for direct interaction or adding new datasets.\n\nDirectory Structure: Datasets are stored under a root directory (default: ~/.identibench_data, configurable via the IDENTIBENCH_DATA_ROOT environment variable). The structure follows: DATA_ROOT / [dataset_id] / [subset] / [experiment_file.hdf5].\nSubsets: Standard subset names are train, valid, and test. An optional train_valid directory might contain combined data.\nDownload & Cache: Data is downloaded automatically when a benchmark requires it and cached locally to avoid re-downloads. The identibench.datasets.download_all_datasets function can fetch all datasets at once.\nFile Format: Processed time-series data is stored in the HDF5 (.hdf5) format.\nHDF5 Structure:\n\nEach .hdf5 file typically represents one experimental run.\nSignals (inputs, outputs, states) are stored as separate 1-dimensional datasets within the file, named conventionally as u0, u1, …, y0, y1, …, x0, …\nData is usually stored as float32 NumPy arrays.\nMetadata like sampling frequency (fs) and suggested initialization window size (init_sz) are stored as attributes on the root group of the HDF5 file.\nExample Structure: my_dataset/       └── train/           └── train_run_1.hdf5               ├── u0 (Dataset: shape=(N,), dtype=float32)               ├── y0 (Dataset: shape=(N,), dtype=float32)               └── Attributes:                   └── fs (Attribute: float)\n\nExtensibility: Adhering to this HDF5 format ensures compatibility when adding new dataset loaders. Helper functions like identibench.utils.write_array facilitate creating files in the correct format.\n\n\n\nUnderstanding Benchmark Results\nThe run_benchmark function returns a dictionary containing detailed results of the experiment. Key entries include:\n\nbenchmark_name (str): The unique name of the benchmark specification used.\ndataset_id (str): Identifier for the dataset source.\nhyperparameters (dict): The hyperparameters dictionary passed to the run.\nseed (int): The random seed used for the run.\ntraining_time_seconds (float): Wall-clock time spent inside your build_model function.\ntest_time_seconds (float): Wall-clock time spent evaluating the returned predictor on the test set.\nbenchmark_type (str): The type of benchmark run (e.g., 'BenchmarkSpecSimulation').\nmetric_name (str): The name of the primary metric function defined in the spec.\nmetric_score (float): The calculated score for the primary metric on the test set (aggregated if multiple test files).\ncustom_scores (dict): Any additional scores calculated by custom evaluation logic specific to the benchmark.\nmodel_predictions (list): A list containing the raw outputs. For simulation, it’s typically [(y_pred_test1, y_true_test1), (y_pred_test2, y_true_test2), ...]. For prediction, the structure might be nested reflecting windowed predictions.",
    "crumbs": [
      "IdentiBench"
    ]
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Benchmark",
    "section": "",
    "text": "source\n\n\n\n BenchmarkSpecSimulation (name:str, dataset_id:str, u_cols:list[str],\n                          y_cols:list[str], metric_func:collections.abc.Ca\n                          llable[[numpy.ndarray,numpy.ndarray],float],\n                          x_cols:list[str]|None=None,\n                          sampling_time:float|None=None, download_func:col\n                          lections.abc.Callable[[pathlib.Path,bool],None]|\n                          None=None, test_model_func:collections.abc.Calla\n                          ble[[__main__.BenchmarkSpecBase,collections.abc.\n                          Callable],dict[str,typing.Any]]=&lt;function\n                          _test_simulation&gt;, custom_test_evaluation=None,\n                          init_window:int|None=None, data_root:[&lt;class'pat\n                          hlib.Path'&gt;,collections.abc.Callable[[],pathlib.\n                          Path]]=&lt;function get_default_data_root&gt;)\n\n*Specification for a simulation benchmark task.\nInherits common parameters from BaseBenchmarkSpec. Use this when the goal is to simulate the system’s output given the input u.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nUnique name identifying this benchmark task.\n\n\ndataset_id\nstr\n\nIdentifier for the raw dataset source.\n\n\nu_cols\nlist\n\nlist of column names for input signals (u).\n\n\ny_cols\nlist\n\nlist of column names for output signals (y).\n\n\nmetric_func\nCallable\n\nPrimary metric: func(y_true, y_pred).\n\n\nx_cols\nlist[str] | None\nNone\nOptional state inputs (x).\n\n\nsampling_time\nfloat | None\nNone\nOptional sampling time (seconds).\n\n\ndownload_func\ncollections.abc.Callable[[pathlib.Path, bool], None] | None\nNone\nDataset preparation func.\n\n\ntest_model_func\nCallable\n_test_simulation\n\n\n\ncustom_test_evaluation\nNoneType\nNone\n\n\n\ninit_window\nint | None\nNone\nSteps for warm-up, potentially ignored in evaluation.\n\n\ndata_root\n[&lt;class ‘pathlib.Path’&gt;, collections.abc.Callable[[], pathlib.Path]]\nget_default_data_root\nroot dir for dataset, may be a callable or path\n\n\n\n\nsource\n\n\n\n\n BenchmarkSpecPrediction (name:str, dataset_id:str, u_cols:list[str],\n                          y_cols:list[str], metric_func:collections.abc.Ca\n                          llable[[numpy.ndarray,numpy.ndarray],float],\n                          pred_horizon:int, pred_step:int,\n                          x_cols:list[str]|None=None,\n                          sampling_time:float|None=None, download_func:col\n                          lections.abc.Callable[[pathlib.Path,bool],None]|\n                          None=None, test_model_func:collections.abc.Calla\n                          ble[[__main__.BenchmarkSpecBase,collections.abc.\n                          Callable],dict[str,typing.Any]]=&lt;function\n                          _test_prediction&gt;, custom_test_evaluation=None,\n                          init_window:int|None=None, data_root:[&lt;class'pat\n                          hlib.Path'&gt;,collections.abc.Callable[[],pathlib.\n                          Path]]=&lt;function get_default_data_root&gt;)\n\n*Specification for a k-step ahead prediction benchmark task.\nInherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones. Use this when the goal is to predict y some steps ahead based on past u and y.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nUnique name identifying this benchmark task.\n\n\ndataset_id\nstr\n\nIdentifier for the raw dataset source.\n\n\nu_cols\nlist\n\nlist of column names for input signals (u).\n\n\ny_cols\nlist\n\nlist of column names for output signals (y).\n\n\nmetric_func\nCallable\n\nPrimary metric: func(y_true, y_pred).\n\n\npred_horizon\nint\n\nThe ‘k’ in k-step ahead prediction (mandatory for this type).\n\n\npred_step\nint\n\nStep size for k-step ahead prediction (e.g., predict y[t+k] using data up to t).\n\n\nx_cols\nlist[str] | None\nNone\nOptional state inputs (x).\n\n\nsampling_time\nfloat | None\nNone\nOptional sampling time (seconds).\n\n\ndownload_func\ncollections.abc.Callable[[pathlib.Path, bool], None] | None\nNone\nDataset preparation func.\n\n\ntest_model_func\nCallable\n_test_prediction\n\n\n\ncustom_test_evaluation\nNoneType\nNone\n\n\n\ninit_window\nint | None\nNone\nSteps for warm-up, potentially ignored in evaluation.\n\n\ndata_root\n[&lt;class ‘pathlib.Path’&gt;, collections.abc.Callable[[], pathlib.Path]]\nget_default_data_root\nroot dir for dataset, may be a callable or path\n\n\n\n\n# Test: BenchmarkSpec basic initialization and defaults\n_spec_sim = BenchmarkSpecSimulation(\n    name='_spec_default', dataset_id='_dummy_default',\n    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n    download_func=_dummy_dataset_loader\n)\ntest_eq(_spec_sim.init_window, None)\ntest_eq(_spec_sim.name, '_spec_default')\n\n\n# Test: BenchmarkSpec initialization with prediction-related parameters\n_spec_pred = BenchmarkSpecPrediction(\n    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n    download_func=_dummy_dataset_loader, \n    init_window=20, pred_horizon=5, pred_step=2\n)\ntest_eq(_spec_pred.init_window, 20)\ntest_eq(_spec_pred.pred_horizon, 5)\ntest_eq(_spec_pred.pred_step, 2)\n\n\n# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n_spec_ensure = BenchmarkSpecSimulation(\n    name='_spec_ensure', dataset_id='_dummy_ensure',\n    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n    download_func=_dummy_dataset_loader\n)\n_spec_ensure.ensure_dataset_exists()\n_dataset_path_ensure = _spec_ensure.dataset_path\ntest_eq(_dataset_path_ensure.is_dir(), True)\ntest_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)\n\n\n# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntime.sleep(0.1) \n_spec_ensure.ensure_dataset_exists() \n_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntest_eq(_mtime_before_skip, _mtime_after_skip)\n\n\n# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntime.sleep(0.1) \n_spec_ensure.ensure_dataset_exists(force_download=True) \n_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntest_ne(_mtime_before_force, _mtime_after_force)\n\nPreparing dataset for '_spec_ensure' at /Users/daniel/.identibench_data/_dummy_ensure...\nDataset '_spec_ensure' prepared successfully.",
    "crumbs": [
      "Benchmark"
    ]
  },
  {
    "objectID": "benchmark.html#benchmark-specifications",
    "href": "benchmark.html#benchmark-specifications",
    "title": "Benchmark",
    "section": "",
    "text": "source\n\n\n\n BenchmarkSpecSimulation (name:str, dataset_id:str, u_cols:list[str],\n                          y_cols:list[str], metric_func:collections.abc.Ca\n                          llable[[numpy.ndarray,numpy.ndarray],float],\n                          x_cols:list[str]|None=None,\n                          sampling_time:float|None=None, download_func:col\n                          lections.abc.Callable[[pathlib.Path,bool],None]|\n                          None=None, test_model_func:collections.abc.Calla\n                          ble[[__main__.BenchmarkSpecBase,collections.abc.\n                          Callable],dict[str,typing.Any]]=&lt;function\n                          _test_simulation&gt;, custom_test_evaluation=None,\n                          init_window:int|None=None, data_root:[&lt;class'pat\n                          hlib.Path'&gt;,collections.abc.Callable[[],pathlib.\n                          Path]]=&lt;function get_default_data_root&gt;)\n\n*Specification for a simulation benchmark task.\nInherits common parameters from BaseBenchmarkSpec. Use this when the goal is to simulate the system’s output given the input u.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nUnique name identifying this benchmark task.\n\n\ndataset_id\nstr\n\nIdentifier for the raw dataset source.\n\n\nu_cols\nlist\n\nlist of column names for input signals (u).\n\n\ny_cols\nlist\n\nlist of column names for output signals (y).\n\n\nmetric_func\nCallable\n\nPrimary metric: func(y_true, y_pred).\n\n\nx_cols\nlist[str] | None\nNone\nOptional state inputs (x).\n\n\nsampling_time\nfloat | None\nNone\nOptional sampling time (seconds).\n\n\ndownload_func\ncollections.abc.Callable[[pathlib.Path, bool], None] | None\nNone\nDataset preparation func.\n\n\ntest_model_func\nCallable\n_test_simulation\n\n\n\ncustom_test_evaluation\nNoneType\nNone\n\n\n\ninit_window\nint | None\nNone\nSteps for warm-up, potentially ignored in evaluation.\n\n\ndata_root\n[&lt;class ‘pathlib.Path’&gt;, collections.abc.Callable[[], pathlib.Path]]\nget_default_data_root\nroot dir for dataset, may be a callable or path\n\n\n\n\nsource\n\n\n\n\n BenchmarkSpecPrediction (name:str, dataset_id:str, u_cols:list[str],\n                          y_cols:list[str], metric_func:collections.abc.Ca\n                          llable[[numpy.ndarray,numpy.ndarray],float],\n                          pred_horizon:int, pred_step:int,\n                          x_cols:list[str]|None=None,\n                          sampling_time:float|None=None, download_func:col\n                          lections.abc.Callable[[pathlib.Path,bool],None]|\n                          None=None, test_model_func:collections.abc.Calla\n                          ble[[__main__.BenchmarkSpecBase,collections.abc.\n                          Callable],dict[str,typing.Any]]=&lt;function\n                          _test_prediction&gt;, custom_test_evaluation=None,\n                          init_window:int|None=None, data_root:[&lt;class'pat\n                          hlib.Path'&gt;,collections.abc.Callable[[],pathlib.\n                          Path]]=&lt;function get_default_data_root&gt;)\n\n*Specification for a k-step ahead prediction benchmark task.\nInherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones. Use this when the goal is to predict y some steps ahead based on past u and y.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nUnique name identifying this benchmark task.\n\n\ndataset_id\nstr\n\nIdentifier for the raw dataset source.\n\n\nu_cols\nlist\n\nlist of column names for input signals (u).\n\n\ny_cols\nlist\n\nlist of column names for output signals (y).\n\n\nmetric_func\nCallable\n\nPrimary metric: func(y_true, y_pred).\n\n\npred_horizon\nint\n\nThe ‘k’ in k-step ahead prediction (mandatory for this type).\n\n\npred_step\nint\n\nStep size for k-step ahead prediction (e.g., predict y[t+k] using data up to t).\n\n\nx_cols\nlist[str] | None\nNone\nOptional state inputs (x).\n\n\nsampling_time\nfloat | None\nNone\nOptional sampling time (seconds).\n\n\ndownload_func\ncollections.abc.Callable[[pathlib.Path, bool], None] | None\nNone\nDataset preparation func.\n\n\ntest_model_func\nCallable\n_test_prediction\n\n\n\ncustom_test_evaluation\nNoneType\nNone\n\n\n\ninit_window\nint | None\nNone\nSteps for warm-up, potentially ignored in evaluation.\n\n\ndata_root\n[&lt;class ‘pathlib.Path’&gt;, collections.abc.Callable[[], pathlib.Path]]\nget_default_data_root\nroot dir for dataset, may be a callable or path\n\n\n\n\n# Test: BenchmarkSpec basic initialization and defaults\n_spec_sim = BenchmarkSpecSimulation(\n    name='_spec_default', dataset_id='_dummy_default',\n    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n    download_func=_dummy_dataset_loader\n)\ntest_eq(_spec_sim.init_window, None)\ntest_eq(_spec_sim.name, '_spec_default')\n\n\n# Test: BenchmarkSpec initialization with prediction-related parameters\n_spec_pred = BenchmarkSpecPrediction(\n    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n    download_func=_dummy_dataset_loader, \n    init_window=20, pred_horizon=5, pred_step=2\n)\ntest_eq(_spec_pred.init_window, 20)\ntest_eq(_spec_pred.pred_horizon, 5)\ntest_eq(_spec_pred.pred_step, 2)\n\n\n# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n_spec_ensure = BenchmarkSpecSimulation(\n    name='_spec_ensure', dataset_id='_dummy_ensure',\n    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n    download_func=_dummy_dataset_loader\n)\n_spec_ensure.ensure_dataset_exists()\n_dataset_path_ensure = _spec_ensure.dataset_path\ntest_eq(_dataset_path_ensure.is_dir(), True)\ntest_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)\n\n\n# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntime.sleep(0.1) \n_spec_ensure.ensure_dataset_exists() \n_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntest_eq(_mtime_before_skip, _mtime_after_skip)\n\n\n# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntime.sleep(0.1) \n_spec_ensure.ensure_dataset_exists(force_download=True) \n_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\ntest_ne(_mtime_before_force, _mtime_after_force)\n\nPreparing dataset for '_spec_ensure' at /Users/daniel/.identibench_data/_dummy_ensure...\nDataset '_spec_ensure' prepared successfully.",
    "crumbs": [
      "Benchmark"
    ]
  },
  {
    "objectID": "benchmark.html#training-context",
    "href": "benchmark.html#training-context",
    "title": "Benchmark",
    "section": "Training Context",
    "text": "Training Context\n\nsource\n\nTrainingContext\n\n TrainingContext (spec:__main__.BenchmarkSpecBase,\n                  hyperparameters:dict[str,typing.Any],\n                  seed:int|None=None)\n\n*Context object passed to the user’s training function (build_predictor).\nHolds the benchmark specification, hyperparameters, and seed. Provides methods to access the raw, full-length training and validation data sequences. Windowing/batching for training must be handled within the user’s build_predictor function.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspec\nBenchmarkSpecBase\n\nThe benchmark specification.\n\n\nhyperparameters\ndict\n\nUser-provided dictionary containing model and training hyperparameters.\n\n\nseed\nint | None\nNone\nOptional random seed for reproducibility.\n\n\n\n\n#todo: test",
    "crumbs": [
      "Benchmark"
    ]
  },
  {
    "objectID": "benchmark.html#benchmark-runtime",
    "href": "benchmark.html#benchmark-runtime",
    "title": "Benchmark",
    "section": "Benchmark Runtime",
    "text": "Benchmark Runtime\n\nsource\n\nrun_benchmark\n\n run_benchmark (spec, build_model, hyperparameters={}, seed=None)\n\n\n# Example usage of run_benchmark\nhyperparams = {'learning_rate': 0.01, 'epochs': 5} # Example hyperparameters\n\nbenchmark_results = run_benchmark(\n    spec=_spec_sim, \n    build_model=_dummy_build_model,\n    hyperparameters=hyperparams\n)\n\nBuilding model with spec: _spec_default, seed: 138830228\n\n\n\n\n{'benchmark_name': '_spec_default',\n 'dataset_id': '_dummy_default',\n 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5},\n 'seed': 138830228,\n 'training_time_seconds': 4.279200220480561e-05,\n 'test_time_seconds': 0.0013009580434300005,\n 'benchmark_type': 'BenchmarkSpecSimulation',\n 'metric_name': 'rmse',\n 'metric_score': 0.5644842382745956,\n 'custom_scores': {}}\n\n\n\n# Example usage of run_benchmark\nbenchmark_results = run_benchmark(\n    spec=_spec_pred, \n    build_model=_dummy_build_model,\n    hyperparameters=hyperparams\n)\n\nBuilding model with spec: _spec_pred_params, seed: 3900254360\n\n\n\n\n{'benchmark_name': '_spec_pred_params',\n 'dataset_id': '_dummy_pred_params',\n 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5},\n 'seed': 3900254360,\n 'training_time_seconds': 6.71250163577497e-05,\n 'test_time_seconds': 0.0010067080147564411,\n 'benchmark_type': 'BenchmarkSpecPrediction',\n 'metric_name': 'rmse',\n 'metric_score': 0.5594019958882623,\n 'custom_scores': {}}\n\n\n\ndef custom_evaluation(results,spec):\n    def get_max_abs_error(y_pred,y_test):\n        return np.max(np.abs(y_test - y_pred))\n    def get_max_error(y_pred,y_test):\n        return np.max(y_test - y_pred)\n\n    avg_max_abs_error = aggregate_metric_score(results, get_max_abs_error, score_name='avg_max_abs_error',sequence_aggregation_func=np.mean,window_aggregation_func=np.mean)\n    median_max_error = aggregate_metric_score(results, get_max_error, score_name='median_max_abs_error',sequence_aggregation_func=np.median,window_aggregation_func=np.median)\n    return {**avg_max_abs_error, **median_max_error}\n\n\nspec_with_custom_test = BenchmarkSpecSimulation(\n    name=\"CustomTestExampleBench\",\n    dataset_id=\"dummy_core_data_v1\", # Same dataset ID as before\n    download_func=_dummy_dataset_loader, \n    u_cols=['u0', 'u1'], \n    y_cols=['y0'],\n    custom_test_evaluation=custom_evaluation,\n    metric_func=identibench.metrics.rmse\n)\n\n\n# Run benchmark using the spec with the custom test function\nhyperparams = {'model_type': 'dummy_v2'} \n\nbenchmark_results = run_benchmark(\n    spec=spec_with_custom_test, \n    build_model=_dummy_build_model,\n    hyperparameters=hyperparams\n)\n\nBuilding model with spec: CustomTestExampleBench, seed: 1172241199\n\n\n\n\n{'benchmark_name': 'CustomTestExampleBench',\n 'dataset_id': 'dummy_core_data_v1',\n 'hyperparameters': {'model_type': 'dummy_v2'},\n 'seed': 1172241199,\n 'training_time_seconds': 2.1415995433926582e-05,\n 'test_time_seconds': 0.0015841670101508498,\n 'benchmark_type': 'BenchmarkSpecSimulation',\n 'metric_name': 'rmse',\n 'metric_score': 0.5739597924041242,\n 'custom_scores': {'avg_max_abs_error': 0.9934645593166351,\n  'median_max_abs_error': 0.9934645593166351}}\n\n\n\nsource\n\n\nbenchmark_results_to_dataframe\n\n benchmark_results_to_dataframe (results_list:list[dict[str,typing.Any]])\n\nTransforms a list of benchmark result dictionaries into a pandas DataFrame.\n\n\n\n\nType\nDetails\n\n\n\n\nresults_list\nlist\nList of benchmark result dictionaries from run_benchmark.\n\n\nReturns\nDataFrame\n\n\n\n\n\nsource\n\n\nrun_benchmarks\n\n run_benchmarks (specs:list[__main__.BenchmarkSpecBase]|dict[str,__main__.\n                 BenchmarkSpecBase], build_model:collections.abc.Callable[\n                 [__main__.TrainingContext],collections.abc.Callable], hyp\n                 erparameters:dict[str,typing.Any]|list[dict[str,typing.An\n                 y]]|None=None, n_times:int=1,\n                 continue_on_error:bool=True, return_dataframe:bool=True)\n\n*Runs multiple benchmarks sequentially, with repetitions and flexible hyperparameters.\nReturns either a pandas DataFrame summarizing the results (default) or a list of raw result dictionaries.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspecs\nlist[main.BenchmarkSpecBase] | dict[str, main.BenchmarkSpecBase]\n\nCollection of specs to run.\n\n\nbuild_model\nCallable\n\nUser function to build the model/predictor.\n\n\nhyperparameters\ndict[str, typing.Any] | list[dict[str, typing.Any]] | None\nNone\nSingle dict, list of dicts (matching specs), or None.\n\n\nn_times\nint\n1\nNumber of times to repeat each benchmark specification.\n\n\ncontinue_on_error\nbool\nTrue\nIf True, continue running benchmarks even if one fails.\n\n\nreturn_dataframe\nbool\nTrue\nIf True, return results as a pandas DataFrame, otherwise return a list of dicts.\n\n\nReturns\npandas.core.frame.DataFrame | list[dict[str, typing.Any]]\n\n\n\n\n\n\nbenchmark_results = run_benchmarks(\n    specs=[_spec_sim,_spec_pred,spec_with_custom_test], \n    build_model=_dummy_build_model,\n    return_dataframe=False\n)\nbenchmark_results_to_dataframe(benchmark_results)\n\n--- Starting benchmark run for 3 specifications, repeating each 1 times ---\n\n-- Repetition 1/1 --\n\n[1/3] Running: _spec_default (Rep 1)\nBuilding model with spec: _spec_default, seed: 2979218856\n  -&gt; Success: _spec_default (Rep 1) completed.\n\n[2/3] Running: _spec_pred_params (Rep 1)\nBuilding model with spec: _spec_pred_params, seed: 2767908549\n  -&gt; Success: _spec_pred_params (Rep 1) completed.\n\n[3/3] Running: CustomTestExampleBench (Rep 1)\nBuilding model with spec: CustomTestExampleBench, seed: 3139743514\n  -&gt; Success: CustomTestExampleBench (Rep 1) completed.\n\n--- Benchmark run finished. 3/3 individual runs completed successfully. ---\n\n\n\n\n\n\n\n\n\nbenchmark_name\ndataset_id\nhyperparameters\nseed\ntraining_time_seconds\ntest_time_seconds\nbenchmark_type\nmetric_name\nmetric_score\ncs_avg_max_abs_error\ncs_median_max_abs_error\n\n\n\n\n0\n_spec_default\n_dummy_default\n{}\n2979218856\n0.000006\n0.001325\nBenchmarkSpecSimulation\nrmse\n0.564484\nNaN\nNaN\n\n\n1\n_spec_pred_params\n_dummy_pred_params\n{}\n2767908549\n0.000006\n0.000844\nBenchmarkSpecPrediction\nrmse\n0.559402\nNaN\nNaN\n\n\n2\nCustomTestExampleBench\ndummy_core_data_v1\n{}\n3139743514\n0.000005\n0.000521\nBenchmarkSpecSimulation\nrmse\n0.573960\n0.993465\n0.993465\n\n\n\n\n\n\n\n\nresults_multiple_runs = run_benchmarks(\n    specs=[_spec_sim,_spec_pred,spec_with_custom_test], \n    build_model=_dummy_build_model,\n    n_times=3\n)\nresults_multiple_runs\n\n--- Starting benchmark run for 3 specifications, repeating each 3 times ---\n\n-- Repetition 1/3 --\n\n[1/9] Running: _spec_default (Rep 1)\nBuilding model with spec: _spec_default, seed: 30935737\n  -&gt; Success: _spec_default (Rep 1) completed.\n\n[2/9] Running: _spec_pred_params (Rep 1)\nBuilding model with spec: _spec_pred_params, seed: 2986847840\n  -&gt; Success: _spec_pred_params (Rep 1) completed.\n\n[3/9] Running: CustomTestExampleBench (Rep 1)\nBuilding model with spec: CustomTestExampleBench, seed: 1147267216\n  -&gt; Success: CustomTestExampleBench (Rep 1) completed.\n\n-- Repetition 2/3 --\n\n[4/9] Running: _spec_default (Rep 2)\nBuilding model with spec: _spec_default, seed: 3191904871\n  -&gt; Success: _spec_default (Rep 2) completed.\n\n[5/9] Running: _spec_pred_params (Rep 2)\nBuilding model with spec: _spec_pred_params, seed: 1536587039\n  -&gt; Success: _spec_pred_params (Rep 2) completed.\n\n[6/9] Running: CustomTestExampleBench (Rep 2)\nBuilding model with spec: CustomTestExampleBench, seed: 3900899545\n  -&gt; Success: CustomTestExampleBench (Rep 2) completed.\n\n-- Repetition 3/3 --\n\n[7/9] Running: _spec_default (Rep 3)\nBuilding model with spec: _spec_default, seed: 3797015292\n  -&gt; Success: _spec_default (Rep 3) completed.\n\n[8/9] Running: _spec_pred_params (Rep 3)\nBuilding model with spec: _spec_pred_params, seed: 3789263585\n  -&gt; Success: _spec_pred_params (Rep 3) completed.\n\n[9/9] Running: CustomTestExampleBench (Rep 3)\nBuilding model with spec: CustomTestExampleBench, seed: 851966748\n  -&gt; Success: CustomTestExampleBench (Rep 3) completed.\n\n--- Benchmark run finished. 9/9 individual runs completed successfully. ---\n\n\n\n\n\n\n\n\n\nbenchmark_name\ndataset_id\nhyperparameters\nseed\ntraining_time_seconds\ntest_time_seconds\nbenchmark_type\nmetric_name\nmetric_score\ncs_avg_max_abs_error\ncs_median_max_abs_error\n\n\n\n\n0\n_spec_default\n_dummy_default\n{}\n30935737\n0.000009\n0.001040\nBenchmarkSpecSimulation\nrmse\n0.564484\nNaN\nNaN\n\n\n1\n_spec_pred_params\n_dummy_pred_params\n{}\n2986847840\n0.000004\n0.000537\nBenchmarkSpecPrediction\nrmse\n0.559402\nNaN\nNaN\n\n\n2\nCustomTestExampleBench\ndummy_core_data_v1\n{}\n1147267216\n0.000004\n0.000385\nBenchmarkSpecSimulation\nrmse\n0.573960\n0.993465\n0.993465\n\n\n3\n_spec_default\n_dummy_default\n{}\n3191904871\n0.000003\n0.000280\nBenchmarkSpecSimulation\nrmse\n0.564484\nNaN\nNaN\n\n\n4\n_spec_pred_params\n_dummy_pred_params\n{}\n1536587039\n0.000003\n0.000285\nBenchmarkSpecPrediction\nrmse\n0.559402\nNaN\nNaN\n\n\n5\nCustomTestExampleBench\ndummy_core_data_v1\n{}\n3900899545\n0.000003\n0.000330\nBenchmarkSpecSimulation\nrmse\n0.573960\n0.993465\n0.993465\n\n\n6\n_spec_default\n_dummy_default\n{}\n3797015292\n0.000003\n0.000264\nBenchmarkSpecSimulation\nrmse\n0.564484\nNaN\nNaN\n\n\n7\n_spec_pred_params\n_dummy_pred_params\n{}\n3789263585\n0.000003\n0.000278\nBenchmarkSpecPrediction\nrmse\n0.559402\nNaN\nNaN\n\n\n8\nCustomTestExampleBench\ndummy_core_data_v1\n{}\n851966748\n0.000003\n0.000531\nBenchmarkSpecSimulation\nrmse\n0.573960\n0.993465\n0.993465\n\n\n\n\n\n\n\n\nsource\n\n\naggregate_benchmark_results\n\n aggregate_benchmark_results (results_df:pandas.core.frame.DataFrame,\n                              group_by_cols:str|list[str]='benchmark_name'\n                              , agg_funcs:str|list[str]='mean')\n\nAggregates numeric results from a benchmark DataFrame, grouped by specified columns.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nresults_df\nDataFrame\n\nDataFrame returned by run_benchmarks (with return_dataframe=True).\n\n\ngroup_by_cols\nstr | list[str]\nbenchmark_name\nColumn(s) to group by before aggregation.\n\n\nagg_funcs\nstr | list[str]\nmean\nAggregation function(s) (‘mean’, ‘median’, ‘std’, etc.) or list thereof.\n\n\nReturns\nDataFrame\n\n\n\n\n\n\naggregate_benchmark_results(results_multiple_runs,agg_funcs=['mean','std'])\n\n\n\n\n\n\n\n\ntraining_time_seconds\ntest_time_seconds\nmetric_score\ncs_avg_max_abs_error\ncs_median_max_abs_error\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nbenchmark_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomTestExampleBench\n0.000003\n4.453506e-07\n0.000415\n0.000104\n0.573960\n0.0\n0.993465\n0.0\n0.993465\n0.0\n\n\n_spec_default\n0.000005\n3.395723e-06\n0.000528\n0.000443\n0.564484\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n_spec_pred_params\n0.000003\n3.254011e-07\n0.000367\n0.000147\n0.559402\n0.0\nNaN\nNaN\nNaN\nNaN",
    "crumbs": [
      "Benchmark"
    ]
  },
  {
    "objectID": "datasets/quad_pelican.html",
    "href": "datasets/quad_pelican.html",
    "title": "Quadrotor Pelican Dataset",
    "section": "",
    "text": "source\n\ndl_quad_pelican\n\n dl_quad_pelican (save_path:pathlib.Path, force_download:bool=False,\n                  remove_download:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nremove_download\nbool\nFalse\n\n\n\nReturns\nNone\n\n\n\n\n\n\ntmp_dir = idb.get_default_data_root()\ndl_quad_pelican(tmp_dir / 'quad_pelican')\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkQuadPelican_Simulation, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkQuadPelican_Simulation, seed: 878947819\n\n\n0.5067444513771566\n\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkQuadPelican_Prediction, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkQuadPelican_Prediction, seed: 2768208524\n\n\n0.4127927071400583",
    "crumbs": [
      "datasets",
      "Quadrotor Pelican Dataset"
    ]
  },
  {
    "objectID": "datasets/industrial_robot.html",
    "href": "datasets/industrial_robot.html",
    "title": "Industrial Robot Dataset",
    "section": "",
    "text": "source\n\ndl_robot_forward\n\n dl_robot_forward (save_path:pathlib.Path, force_download:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nReturns\nNone\n\n\n\n\n\n\ntmp_dir = idb.get_default_data_root()\ndl_robot_forward(tmp_dir / 'robot_forward')\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkRobotForward_Simulation, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkRobotForward_Simulation, seed: 2680628891\n\n\n25.958381135555868\n\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkRobotForward_Prediction, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkRobotForward_Prediction, seed: 828977438\n\n\n24.406405307477502\n\n\n\nsource\n\n\ndl_robot_inverse\n\n dl_robot_inverse (save_path:pathlib.Path, force_download:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nFalse\nforce download the dataset\n\n\nReturns\nNone\n\n\n\n\n\n\ntmp_dir = idb.get_default_data_root()\ndl_robot_inverse(tmp_dir / 'robot_inverse')\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkRobotInverse_Simulation, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkRobotInverse_Simulation, seed: 2022869814\n\n\n3.9566372133884067\n\n\n\nresults = idb.run_benchmark(\n    spec=BenchmarkRobotInverse_Prediction, \n    build_model=idb._dummy_build_model\n)\nresults['metric_score']\n\nBuilding model with spec: BenchmarkRobotInverse_Prediction, seed: 3680826223\n\n\n3.900040000420601",
    "crumbs": [
      "datasets",
      "Industrial Robot Dataset"
    ]
  },
  {
    "objectID": "datasets/broad.html",
    "href": "datasets/broad.html",
    "title": "Berlin Robust Orientation Estimation Assessment Dataset (BROAD)",
    "section": "",
    "text": "source\n\ndl_broad\n\n dl_broad (save_path:pathlib.Path, force_download:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nPath\n\ndirectory the files are written to, created if it does not exist\n\n\nforce_download\nbool\nTrue\nforce download the dataset\n\n\nReturns\nNone\n\n\n\n\n\n\ntmp_dir = Path('./tmp')\ndl_broad(tmp_dir / 'broad')",
    "crumbs": [
      "datasets",
      "Berlin Robust Orientation Estimation Assessment Dataset (BROAD)"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\nrmse\n\n rmse (y_true:numpy.ndarray, y_pred:numpy.ndarray, time_axis:int=0)\n\n*Computes the Root Mean Square Error (RMSE) along a specified time axis.\nCalculates RMSE = sqrt(mean((y_pred - y_true)**2)) separately for each channel defined by the remaining axes.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny_true\nndarray\n\nGround truth target values.\n\n\ny_pred\nndarray\n\nEstimated target values.\n\n\ntime_axis\nint\n0\nAxis representing time or samples.\n\n\nReturns\nndarray\n\nRoot Mean Squared Error for each channel.\n\n\n\n\n# Example usage for rmse\n\n# 1D case\ny_t1 = np.array([1, 2, 3, 4, 5])\ny_p1 = np.array([1.1, 2.1, 3.1, 4.1, 5.1]) # Small error\nrmse_val_1d = rmse(y_t1, y_p1)\nprint(f\"RMSE (1D): {rmse_val_1d}\") # Should be ~0.1\n\n# 2D case (e.g., time x features)\ny_t2 = np.array([[1, 10], [2, 20], [3, 30]])\ny_p2 = np.array([[1, 11], [2, 21], [3, 31]]) # Error=0 in col 0, Error=1 in col 1\nrmse_val_2d = rmse(y_t2, y_p2, time_axis=0)\nprint(f\"RMSE (2D, time_axis=0): {rmse_val_2d}\") # Should be [0., 1.]\n\n# 3D case (e.g., batch x time x features)\ny_t3 = np.random.rand(2, 10, 3) # batch=2, time=10, features=3\ny_p3 = y_t3 + np.random.randn(2, 10, 3) * 0.1\nrmse_val_3d = rmse(y_t3, y_p3, time_axis=1) # Calculate RMSE over time axis\nprint(f\"RMSE (3D, time_axis=1, shape={rmse_val_3d.shape}): {rmse_val_3d}\") # Should have shape (2, 3)\n\nRMSE (1D): 0.09999999999999991\nRMSE (2D, time_axis=0): [0. 1.]\nRMSE (3D, time_axis=1, shape=(2, 3)): [[0.09308798 0.11013668 0.11058777]\n [0.08573954 0.06386016 0.06679621]]\n\n\n\nsource\n\n\nnrmse\n\n nrmse (y_true:numpy.ndarray, y_pred:numpy.ndarray, time_axis:int=0,\n        std_tolerance:float=1e-09)\n\n*Computes the Normalized Root Mean Square Error (NRMSE).\nCalculates NRMSE = RMSE / std(y_true) separately for each channel. Returns NaN for channels where std(y_true) is close to zero (below std_tolerance).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny_true\nndarray\n\nGround truth target values.\n\n\ny_pred\nndarray\n\nEstimated target values.\n\n\ntime_axis\nint\n0\nAxis representing time or samples.\n\n\nstd_tolerance\nfloat\n1e-09\nMinimum standard deviation allowed for y_true to avoid division by zero.\n\n\nReturns\nndarray\n\nNormalized Root Mean Squared Error for each channel.\n\n\n\n\n# Example usage for nrmse\n\n# 1D case\ny_t1 = np.array([1, 2, 3, 4, 5])\ny_p1 = y_t1 + 0.1 # Constant offset error\nnrmse_val_1d = nrmse(y_t1, y_p1)\nprint(f\"NRMSE (1D): {nrmse_val_1d}\") # RMSE is 0.1, std(y_t1) is sqrt(2). NRMSE = 0.1 / sqrt(2)\n\n# 2D case\ny_t2 = np.array([[1, 10], [2, 20], [3, 30]])\ny_p2 = np.array([[1, 11], [2, 21], [3, 31]]) # Error=0 in col 0, Error=1 in col 1\n# Std dev col 0: ~0.816, Std dev col 1: ~8.165\n# RMSE col 0: 0, RMSE col 1: 1\n# NRMSE col 0: 0 / 0.816 = 0\n# NRMSE col 1: 1 / 8.165 = ~0.122\nnrmse_val_2d = nrmse(y_t2, y_p2, time_axis=0)\nprint(f\"NRMSE (2D, time_axis=0): {nrmse_val_2d}\") # Should be approx [0., 0.122]\n\n# Case with zero standard deviation\ny_t_const = np.array([5, 5, 5, 5])\ny_p_const = np.array([5, 5, 5, 6]) # RMSE = 0.5\nnrmse_val_const = nrmse(y_t_const, y_p_const)\nprint(f\"NRMSE (Constant y_true): {nrmse_val_const}\") # Should be NaN with a warning\n\nNRMSE (1D): 0.07071067811865468\nNRMSE (2D, time_axis=0): [0.         0.12247449]\nNRMSE (Constant y_true): nan\n\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_17861/2787241071.py:31: RuntimeWarning: Standard deviation of y_true is below tolerance (1e-09) for some channels. NRMSE set to NaN for these channels.\n  warnings.warn(f\"Standard deviation of y_true is below tolerance ({std_tolerance}) for some channels. NRMSE set to NaN for these channels.\", RuntimeWarning)\n\n\n\nsource\n\n\nfit_index\n\n fit_index (y_true:numpy.ndarray, y_pred:numpy.ndarray, time_axis:int=0,\n            std_tolerance:float=1e-09)\n\n*Computes the Fit Index (FIT) commonly used in System Identification.\nCalculates FIT = 100 * (1 - NRMSE) separately for each channel. Returns NaN for channels where NRMSE could not be calculated (e.g., std(y_true) near zero).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny_true\nndarray\n\nGround truth target values.\n\n\ny_pred\nndarray\n\nEstimated target values.\n\n\ntime_axis\nint\n0\nAxis representing time or samples.\n\n\nstd_tolerance\nfloat\n1e-09\nMinimum standard deviation allowed for y_true.\n\n\nReturns\nndarray\n\nFit index (in percent) for each channel.\n\n\n\n\n# Example usage for fit_index\n\n# 1D case (using previous example)\n# NRMSE = 0.1 / sqrt(2) approx 0.0707\n# FIT = 100 * (1 - 0.0707) approx 92.93\nfit_val_1d = fit_index(y_t1, y_p1)\nprint(f\"Fit Index (1D): {fit_val_1d}\")\n\n# 2D case (using previous example)\n# NRMSE approx [0., 0.122]\n# FIT approx [100 * (1 - 0), 100 * (1 - 0.122)] = [100, 87.8]\nfit_val_2d = fit_index(y_t2, y_p2, time_axis=0)\nprint(f\"Fit Index (2D, time_axis=0): {fit_val_2d}\")\n\n# Constant case (using previous example)\n# NRMSE is NaN\n# FIT should also be NaN\nfit_val_const = fit_index(y_t_const, y_p_const)\nprint(f\"Fit Index (Constant y_true): {fit_val_const}\")\n\nFit Index (1D): 92.92893218813452\nFit Index (2D, time_axis=0): [100.          87.75255129]\nFit Index (Constant y_true): nan\n\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_17861/2787241071.py:31: RuntimeWarning: Standard deviation of y_true is below tolerance (1e-09) for some channels. NRMSE set to NaN for these channels.\n  warnings.warn(f\"Standard deviation of y_true is below tolerance ({std_tolerance}) for some channels. NRMSE set to NaN for these channels.\", RuntimeWarning)\n\n\n\nsource\n\n\nmae\n\n mae (y_true:numpy.ndarray, y_pred:numpy.ndarray, time_axis:int=0)\n\n*Computes the Mean Absolute Error (MAE) along a specified time axis.\nCalculates MAE = mean(abs(y_pred - y_true)) separately for each channel defined by the remaining axes.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny_true\nndarray\n\nGround truth target values.\n\n\ny_pred\nndarray\n\nEstimated target values.\n\n\ntime_axis\nint\n0\nAxis representing time or samples.\n\n\nReturns\nndarray\n\nMean Absolute Error for each channel.\n\n\n\n\n# Example usage for mae\n\n# 1D case\ny_t1 = np.array([1, 2, 3, 4, 5])\ny_p1 = np.array([1.1, 2.1, 3.1, 4.1, 5.1]) # Constant error of 0.1\nmae_val_1d = mae(y_t1, y_p1)\nprint(f\"MAE (1D): {mae_val_1d}\") # Should be 0.1\n\ny_p2 = np.array([0, 1, 2, 3, 4]) # Constant error of -1 -&gt; abs error 1\nmae_val_1d_neg = mae(y_t1, y_p2)\nprint(f\"MAE (1D, negative err): {mae_val_1d_neg}\") # Should be 1.0\n\n# 2D case (e.g., time x features)\ny_t2 = np.array([[1, 10], [2, 20], [3, 30]])\ny_p3 = np.array([[1, 11], [2, 21], [3, 31]]) # Error=0 in col 0, Error=1 in col 1\nmae_val_2d = mae(y_t2, y_p3, time_axis=0)\nprint(f\"MAE (2D, time_axis=0): {mae_val_2d}\") # Should be [0., 1.]\n\nMAE (1D): 0.09999999999999991\nMAE (1D, negative err): 1.0\nMAE (2D, time_axis=0): [0. 1.]\n\n\n\nsource\n\n\nr_squared\n\n r_squared (y_true:numpy.ndarray, y_pred:numpy.ndarray, time_axis:int=0,\n            std_tolerance:float=1e-09)\n\n*Computes the R-squared (coefficient of determination) score.\nCalculates R^2 = 1 - NRMSE^2 separately for each channel. Returns NaN for channels where NRMSE could not be calculated (e.g., std(y_true) near zero). A constant model that always predicts the mean of y_true would get R^2=0.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny_true\nndarray\n\nGround truth target values.\n\n\ny_pred\nndarray\n\nEstimated target values.\n\n\ntime_axis\nint\n0\nAxis representing time or samples.\n\n\nstd_tolerance\nfloat\n1e-09\nMinimum standard deviation allowed for y_true.\n\n\nReturns\nndarray\n\nR-squared (coefficient of determination) for each channel.\n\n\n\n\n# Example usage for r_squared\n\n# 1D case (using previous nrmse example)\n# NRMSE = 0.1 / sqrt(2) approx 0.0707\n# R^2 = 1 - (0.0707)^2 approx 1 - 0.005 = 0.995\ny_t1 = np.array([1, 2, 3, 4, 5])\ny_p1 = np.array([1.1, 2.1, 3.1, 4.1, 5.1])\nr2_val_1d = r_squared(y_t1, y_p1)\nprint(f\"R-squared (1D): {r2_val_1d}\")\n\n# Perfect prediction\nr2_perfect = r_squared(y_t1, y_t1)\nprint(f\"R-squared (1D, perfect): {r2_perfect}\") # Should be 1.0\n\n# 2D case (using previous nrmse example)\n# NRMSE approx [0., 0.122]\n# R^2 approx [1 - 0^2, 1 - 0.122^2] = [1, 1 - 0.0149] = [1, 0.9851]\ny_t2 = np.array([[1, 10], [2, 20], [3, 30]])\ny_p2 = np.array([[1, 11], [2, 21], [3, 31]])\nr2_val_2d = r_squared(y_t2, y_p2, time_axis=0)\nprint(f\"R-squared (2D, time_axis=0): {r2_val_2d}\")\n\n# Constant case (using previous nrmse example)\n# NRMSE is NaN\n# R^2 should also be NaN\ny_t_const = np.array([5, 5, 5, 5])\ny_p_const = np.array([5, 5, 5, 6])\nr2_val_const = r_squared(y_t_const, y_p_const)\nprint(f\"R-squared (Constant y_true): {r2_val_const}\") # Should be NaN with a warning from nrmse\n\nR-squared (1D): 0.995\nR-squared (1D, perfect): 1.0\nR-squared (2D, time_axis=0): [1.    0.985]\nR-squared (Constant y_true): nan\n\n\n/var/folders/pc/13zbh_m514n1tp522cx9npt00000gn/T/ipykernel_17861/2787241071.py:31: RuntimeWarning: Standard deviation of y_true is below tolerance (1e-09) for some channels. NRMSE set to NaN for these channels.\n  warnings.warn(f\"Standard deviation of y_true is below tolerance ({std_tolerance}) for some channels. NRMSE set to NaN for these channels.\", RuntimeWarning)",
    "crumbs": [
      "Metrics"
    ]
  }
]